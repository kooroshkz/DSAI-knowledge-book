# **Part 1**
### "Introduction to Socially Interactive Agents" (SIA)

**Key Concepts:**
**Socially Interactive Agents (SIAs)**:
   - Virtually or physically embodied agents that can autonomously interact with humans using multimodal, socially intelligent behavior (verbal, para-verbal, non-verbal).
   - Contain agents like *Intelligent Virtual Agents*, *Embodied Conversational Agents*, and *Social Robots*.

**Terminology**:

| **Term**                         | **Definition**                                                                                                                                      |
|----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Agent**                        | Autonomous entities that perceive and act on their environment, not necessarily embodied.                                                            |
| **Avatar**                       | Graphical representation controlled by a user in a virtual environment, not autonomous.                                                              |
| **Embodied Conversational Agent**| Computer-generated characters that engage in conversations using verbal and non-verbal cues.                                                         |
| **Intelligent Virtual Agent**    | Digital characters with human-like traits, capable of dynamic real-time interaction with humans.                                                     |
| **Socially Assistive Robot**     | Robots providing assistance through social interaction, focusing on social engagement rather than task execution.                                     |
| **Socially Intelligent Agent**   | Agents displaying human-style social intelligence with deep models of cognition and social competence.                                                |
| **Socially Interactive Robot**   | Robots where social interaction is the primary focus, distinguishing them from teleoperation robots.                                                  |
| **Socially Interactive Agent**   | Agents (virtual or physical) capable of human-like social interaction, used in diverse applications like education or healthcare.                     |
| **Social Robot**                 | Robots designed to engage in natural, interpersonal communication with humans using verbal and non-verbal signals.                                    |
| **Virtual Character**            | Virtual representations, typically animated, that can convey emotions but may not be interactive or intelligent.                                      |
| **Virtual Human**                | Artificial characters that look and act like humans in a simulated environment, often with realistic graphics.                                         |


**Origins**:
   - **IVAs** originated from computer graphics advancements, focusing on simulating human behavior.
   - **Social Robots (SRs)** come from robotics, focusing on interpersonal interaction and "robotiquette" to interact with humans.

**Embodiment**:
   - **Virtual Embodiment**: Offers customizable appearance, animations, and a safe interaction setting.
   - **Physical Embodiment**: Offers physical presence, mobility, and the ability to interact with the real world.

**Advantages of SIAs**:
   - Virtual embodiment offers customization and fine-grained animations.
   - Physical embodiment provides mobility, real-world interaction, and increased social presence.

**Challenges in Development**:
- Both virtual and physical SIAs have different strengths, making the choice of embodiment critical based on the task and scenario.
- Current research seeks to balance between the two for optimal interaction.

### Important Topics:
- **Social Cognition**: SIAs must replicate human-like emotions, empathy, and social understanding for natural interaction.
- **Long-Term Interaction**: The ability of SIAs to form long-term relationships and adapt their personalities is key for future developments.
- **Research Gaps**: There's a lack of unified research across SIAs, requiring interdisciplinary collaboration between cognitive sciences, psychology, AI, and robotics.

---

# **Part 2**

**Why Methodological Knowledge is Important**:
- The methods used in social sciences are focused on human behavior, perception, and attitudes, making them essential for evaluating SIAs in real-world scenarios.

**Empirical Method in Social Sciences approaches**:
- **Hypothetico-deductive model**: Begins with a hypothesis and seeks evidence to support or refute it.
- **Inductive reasoning**: Gathers data methodically to develop new theories.

**The Research Process (8 Steps)**:
   1. **Finding a Research Topic**
   2. **Forming a Research Question/Hypothesis**
   3. **Research Strategy and Experiment Design**:
      - Types of research strategies:
        - **Descriptive**: Describes current states of variables.
        - **Correlational**: Examines relationships between variables.
        - **Experimental**: Determines cause-effect relationships by manipulating variables.
      - Experiment designs include:
         - **within-subjects**: Same participants experience all conditions.
         - **between-subjects**: Different participants for each condition.
         - **factorial designs**: Tests multiple variables simultaneously.
   4. **Operationalizing Variables**: Define variables in measurable terms, ensuring their reliability and validity.
   5. **Selecting a Sample**
   6. **Data Collection**
   7. **Data Processing and Analysis**
   8. **Reporting Results**: Write up findings following the structure: **Introduction**, **Methods**, **Results**, and **Discussion** (IMRaD format).

**Types of Studies in SIA Research**:
   - **Interviews**: Gather in-depth qualitative data. Can be structured, semi-structured, or unstructured, depending on the flexibility of the questions.
   - **Perception-only Studies**: Focus on participants’ perceptions without interaction, often used in early stages of development.
   - **Interaction Studies**: Evaluate how users interact with SIAs and measure behavior or outcomes.

### Biases in Interviews:

1. **Experimenter Bias**:
   - Biases can come from both participants and researchers.
   - Researchers may unintentionally influence participants (e.g., due to gender, appearance, or behavior).
   
2. **Suggestion**:
   - Interviewers must avoid suggesting answers, verbally or non-verbally (e.g., nodding, smiling).
   - Questions like "Did you like that?" or "What is your opinion?" can lead to biased responses.

3. **The Good Participant Effect (Good Subject Effect)**:
   - Participants may try to please the interviewer by giving helpful or favorable answers.
   - This effect arises from the trust and connection built between the interviewer and participant.

4. **Introspection**:
   - People often overestimate their ability to know why they feel or behave a certain way.
   - The "introspection illusion" suggests that introspective information may not be fully reliable.

---

**Practical Considerations**:
   - **Ethical standards** when using techniques like the **Wizard-of-Oz**.
   - **Power analysis** the right number of participants needed for your study to make sure the results are reliable.
   - Control biases and **random assignment**.

### Critical Concepts:
- **Operationalization**: Translating abstract concepts (like "trust" or "attention") into measurable variables.
- **Validity & Reliability**: Ensure that the methods used truly measure what they intend (validity) and do so consistently (reliability).
- **Sampling**: The method for selecting participants affects how generalizable the results are. Aim for a representative sample to minimize bias.

---

# **Part 4**

### **Design Approaches**
   - **Metaphorical Design**:
     - **Consistency vs. Inconsistency**: Metaphors can be consistent (appearance and behavior align, like a dog behaving like a dog) or inconsistent (a dog that speaks).


- Research supports the **Embodiment Hypothesis**: Physical or human-scale agents tend to improve user performance and perceptions more than virtual agents.

### **Agent Construction**
   - **Virtual Agents**: 
      - 3D models, 
      - Texture mapping
      - Shading techniques

   - **Physical Agents**: Built using principles from industrial design, considering materials, mechanics, and interaction requirements.

### 6. **Facial and Bodily Features**
   - **Facial Features**: The face is crucial for user interaction, conveying emotion, attention, and trustworthiness.
     - Subtle facial adjustments (e.g., eye size) affect perceptions of trust or aggression.
   - **Body Features**: The agent’s body, including clothing and accessories, contributes to its perceived role and functionality.

---

# **Part 7**

**Chapter 7: Gesture Generation**

### **Classification of Gestures**
   - **Emblems**: Standalone gestures (e.g., thumbs up) that replace speech. Their meaning is culture-dependent.
   - **Beats**: Simple gestures that emphasize speech rhythm without conveying specific meaning.
   - **Iconic gestures**: Represent physical objects or actions (e.g., mimicking cutting with a knife).
   - **Deictic gestures**: Pointing gestures that direct attention to something specific.
   - **Metaphoric gestures**: Represent abstract concepts (e.g., cupping hands to represent "all ideas").

### **Gestural Phases**:
 Rest, preparation, stroke (main part of the gesture), hold, and relax phases. These phases may flow into each other in a gesture sequence.

### 7. **Rule-based vs. Data-driven Approaches**
   - **Rule-based models**: Use predefined rules and linguistic information to generate gestures.
      - **Cerebella Architecture**:
      - A hybrid system that uses both **acoustic** and **linguistic elements** to dynamically generate gestures with both the speech's auditory cues and its semantic structure.
      - Used in both **virtual agent** and **social robotics**.
      - Divides its process into stages:
      1. Input treatment (text and audio processing).
      2. Communicative functions (CFs) like emotion, emphasis analysis.
      3. Behavior mapping based on the context.
      4. Animation scheduling.

   - **GRETA Architecture**:
      - Uses **high-level concepts** and external context (e.g., **dialogue goals** or **user moves**) to influence gesture generation by **communicative intentions**
      
      **Workflow**:
      1. **User Input**: The user speaks or interacts with the agent.
      2. **MIND**: The system processes this input, deciding the agent’s emotional tone and what it should say.
      3. **MIDAS**: Adds details to the agent’s response by selecting appropriate gestures and expressions that match the speech.
      4. **BODY**: The chosen gestures are generated and timed perfectly with the speech for a natural response.
      5. **User Model & Context Features**: Throughout, the system considers the user's preferences, reactions, and the external context to tailor the response, ensuring it fits the conversation setting.
   - **Data-driven models**: Use machine learning to learn gestures from large datasets of audio and video, producing more natural gestures but with less nuanced communication.

---

# **Part 8**

**Chapter 8: Multimodal Behavior Modeling for Socially Interactive Agents**:

### **Nonverbal Behavior Representation**
   - **Coding Schemes**: Systems like FACS (Facial Action Coding System) help standardize the description of facial expressions by breaking them into muscular movements (AUs or Action Units).
   - **Body Coding Systems**: BACS (Body Action Coding System) focuses on body movements and emotions.

### **Models and Approaches**
   - **Rule-based Models**: Early models like GestureJack and REA (Real-Estate Agent) used rules derived from social science to drive the generation of nonverbal behaviors.
   - **Performance-based Models**: Use human recordings (e.g., motion capture) to generate animations, preserving the natural timing of speech and gestures.
   - **Machine Learning Models**: These models use data to learn the relationship between speech and gestures. Techniques include HMMs (Hidden Markov Models), Dynamic Bayesian Networks, and deep learning models like LSTMs and GANs.
### **Challenges in Multimodal Behavior Modeling**
   - Handling the **polysemy** (multiple meanings)
   
### Graphical models, like HMMs and CRFs

Help map temporal relationships between speech and gestures.

### **Key Technologies and Tools**
   - **BML (Behavior Markup Language)**: A tool used to specify and synchronize behaviors (e.g., gestures, facial expressions) in multimodal systems.