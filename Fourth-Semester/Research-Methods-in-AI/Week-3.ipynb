{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51add83",
   "metadata": {},
   "source": [
    "- **The Empirical Cycle**\n",
    "    - **Observation**: You notice something interesting or strange.\n",
    "    - **Induction**: You form a hypothesis based on your observations.\n",
    "    - **Deduction**: Make testable predictions from your hypothesis.\n",
    "    - **Testing**: You conduct experiments or gather data to test your predictions.\n",
    "    - **Evaluation**: Analyze the results to see if they support or refute your hypothesis.\n",
    "\n",
    "- **Fraud** in resarch:\n",
    "    - **Fabrication**: Making up data or results.\n",
    "    - **Falsification**: Manipulating research materials, equipment, or processes to misrepresent results.\n",
    "    - **Plagiarism**: Using someone else's work or ideas without proper attribution.\n",
    "\n",
    "- **Questionable research practices (QRPs)**\n",
    "    - **Inappropriate publication practices**: How you publish and present results\n",
    "        - **Salami slicing**: Publishing multiple papers from the same dataset (Wastes journal space, overstates how much evidence exists.)\n",
    "        - **HARKing** (Hypothesizing After the Results are Known): Turns an exploratory finding into a fake “confirmation”.\n",
    "            - **Exploratory**: When you explore data to find patterns, you’re generating new ideas or hypotheses.\n",
    "            - **Confirmatory**: Testing a hypothesis you made before seeing the data.\n",
    "        - **Selective reporting**: Only reporting results that support your hypothesis (Gives a false success rate; readers think the effect is stronger than it really is.)\n",
    "        - **Cherry-picking**: Selectively reporting results that support your hypothesis\n",
    "    - **Messing up the empirical cycle**: Skipping or re-using steps of the research process\n",
    "        - **re-testing old data without new data**: Using old data to test new hypotheses without collecting new data (results aren’t truly confirmed.)\n",
    "    - **P-hacking**: Tweaking the analysis until the p-value < 0.05\n",
    "        - **rounding p**: Rounding p-values to make them appear significant\n",
    "        - **Adjusting outlier criteria**: Altering how outliers are defined to affect results. Optimal ways:\n",
    "            - Outliers are values above Q3 + 1.5 × IQR or below Q1 − 1.5 × IQR\n",
    "            - Values more than 3 or 2.5 standard deviations from the mean\n",
    "        - **Selecting levels of the independent variable**: Test only a subset of conditions that show an effect.\n",
    "        - **Selecting from multiple dependent variables**: Measure ten outcomes but only report the one that’s significant. $\\to$ false positive\n",
    "        - **Adding/removing covariates**: Try different variable combination and report the one that gives a significant result.\n",
    "        - **Sequential testing with optional stopping**: Adding samples and check p-value till somewhere drops below 0.05 and stop adding samples.\n",
    "            - It start from low sample size and increase it until the p-value is significant, so not always increase sample size.\n",
    "\n",
    "- P-hacing is like overfitting in ML, the same method in ML is called 'performance hacking'.\n",
    "- Optional stopping $\\to$ Error typ I $\\to$ False positive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
