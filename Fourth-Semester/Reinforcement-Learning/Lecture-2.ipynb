{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                   | Dataset Given          | Actively Collect Data             |\n",
    "|-------------------|------------------------|-----------------------------------|\n",
    "| **Full feedback**    | Supervised learning   | (Active supervised learning)       |\n",
    "| **Partial feedback** | (Offline RL)          | Bandits / Reinforcement learning   |\n",
    "| **No feedback**      | Unsupervised learning | (Active unsupervised learning)     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Bandits\n",
    "\n",
    "In **Bandits**, each action (arm pull) leads to a single reward, and the environment doesn't evolve into a new state. \n",
    "\n",
    "Unlike standard RL, where actions change the state and influence future rewards, bandit environments treat every pull as an independent one-step problem. \n",
    "\n",
    "By balancing exploration (trying different arms) and exploitation (pulling the best-known arm), RL approaches maximize long-term reward despite uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. **Bandit Definition**\n",
    "A *bandit* is defined by the tuple $ \\langle A, p(r|a) \\rangle $:\n",
    "- $ A $: A set of discrete actions (also called arms).\n",
    "- $ p(r|a) $: A conditional probability distribution that maps each action $ a $ to a distribution over possible rewards $ r $.\n",
    "\n",
    "### 2. **Action Value Q(a)** or **Expected reward of an action $ a $**\n",
    "$$\n",
    "Q(a) = \\mathbb{E}_{r \\sim p(r|a)}[r]\n",
    "$$\n",
    "\n",
    "Where denotes the expected value of R based on probability of reward $ r $ if we take action $a$.\n",
    "\n",
    "#### Initialization of $Q(a)$\n",
    "\n",
    "- **Realistic Initialization**:\n",
    "\n",
    "    $ Q(a)=0 , ∀ a∈A $ :    Start with initial guess that the reward for each action is 0.\n",
    "- **Optimistic Initialization**:\n",
    "\n",
    "    $ Q(a)=ψ , ∀a∈A$ : $ψ$ is a positive hyperparameter. This initialization encourages exploration by starting with a high value for each action. This makes the algorithm more likely to explore less tried actions, since they appear to have high expected rewards initially.\n",
    "\n",
    "### 3. **Policy ($π$)** or **Action selection**\n",
    "Gives probability of Choosing an Action based on the policy which consider Q(a).\n",
    "\n",
    "### 4. **Objective (Maximizing Cumulative Reward)**\n",
    "\n",
    "The goal of the bandit problem is to maximize the total reward over a given number of time steps $ T $. At each time step $ t $, an action $ a_t $ is chosen according to the policy $ \\pi $, and a reward $ r_t $ is observed from the distribution $ p(r | a_t) $. The objective function $ J_T(\\pi) $ represents the expected total reward over the time horizon $ T $, and is given by:\n",
    "\n",
    "![Objective](../../Files/fourth-semester/rl/2.png)\n",
    "\n",
    "The goal is to find the optimal policy $ \\pi^* $ that maximizes this expected cumulative reward:\n",
    "\n",
    "$$\n",
    "\\pi^* = \\arg\\max_{\\pi} J_T(\\pi)\n",
    "$$\n",
    "\n",
    "This means we are trying to find the policy that leads to the highest expected total reward after $ T $ timesteps.\n",
    "\n",
    "### **Bandit Algorithm Pseudocode**\n",
    "\n",
    "```markdown\n",
    "Input: Maximum number of timesteps T\n",
    "Initialize policy π(a)\n",
    "\n",
    "for t = 1...T:\n",
    "    at ∼ π(a)     # Sample an action from the policy -> Exploration/exploitation!\n",
    "    rt ∼ p(r|at)  # Observe the reward for that action\n",
    "    Update π based on (at, rt)   # Update the policy with the new information\n",
    "\n",
    "End\n",
    "```\n",
    "\n",
    "In summary:\n",
    "- **Bandit Problem**: Involves selecting actions (arms) and receiving rewards.\n",
    "- **Action Value (Q(a))**: The expected reward for each action.\n",
    "- **Policy (π)**: A strategy that dictates which actions to take.\n",
    "- **Objective**: Maximize the cumulative reward over time.\n",
    "\n",
    "The challenge is to estimate the action values efficiently and adjust the policy to maximize long-term rewards.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Estimate a mean\n",
    "\n",
    "Many algorithms rely on our ability to estimate the mean reward of an action:\n",
    "\n",
    "However, now next $r_{n+1}$ comes in, and how do we update the current mean $Q_n$ ?\n",
    "- **Incremental update**\n",
    "- **Learning update**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Q(n) = 1/n \\sum_{i=1}^{n} r_i $ is the mean of the first n rewards but now we want to make formula how to update the mean when we get new reward $r_{n+1}$\n",
    "- **Incremental update**:\n",
    "$$ Q_{n} = Q_{n-1} + \\frac{1}{n} [r_{n} - Q_{n-1}] $$\n",
    "- **Learning update**:\n",
    "\n",
    "Simply move the new mean a bit in the direction of the last observed reward\n",
    "$$ Q_n \\leftarrow Q_{n-1} + \\alpha \\left[ r_n - Q_{n-1} \\right]\n",
    " $$\n",
    "\n",
    "Where $ \\alpha $ is the learning rate, which controls how much the new reward influences the current estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **Bandit algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Policies\n",
    "\n",
    "### **Greedy Policy (with Optimistic Initialization)**:\n",
    "  - The greedy policy selects the action with the highest $ Q(a) $ value.\n",
    "  - **Mathematically**:\n",
    "    $$\n",
    "    \\pi_{\\text{greedy}}(a) = \\begin{cases} \n",
    "    1, & \\text{if } a = \\arg\\max_{b \\in A} Q(b) \\\\\n",
    "    0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    This selects the best-known action with certainty. Optimistic initialization ensures that initially all actions are explored equally by setting high initial estimates for $ Q(a) $.\n",
    "\n",
    "![](../../Files/fourth-semester/rl/5.png)\n",
    "\n",
    "\n",
    "### **$ \\epsilon $-Greedy Policy**:\n",
    "  - Balances exploration and exploitation by choosing the best action with probability $ 1 - \\epsilon $ and exploring other actions randomly with probability $ \\epsilon $.\n",
    "  - **Mathematically**:\n",
    "    $$\n",
    "    \\pi_{\\epsilon\\text{-greedy}}(a) = \\begin{cases}\n",
    "    1 - \\epsilon, & \\text{if } a = \\arg\\max_{b \\in A} Q(b) \\\\\n",
    "    \\frac{\\epsilon}{|A| - 1}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "\n",
    "**Note**: $ε$ = exploration parameter which scale the amount of exploration\n",
    "\n",
    "![](../../Files/fourth-semester/rl/3.png)\n",
    "\n",
    "![](../../Files/fourth-semester/rl/4.png)\n",
    "  \n",
    "### **Softmax (Boltzmann) Policy**:\n",
    "  - Uses a probabilistic approach where actions with higher $ Q(a) $ values are more likely to be chosen.\n",
    "  - **Mathematically**:\n",
    "    $$\n",
    "    \\pi_{\\text{softmax}}(a) = \\frac{\\exp(Q(a)/\\tau)}{\\sum_{b \\in A} \\exp(Q(b)/\\tau)}\n",
    "    $$\n",
    "    Where $ \\tau $ (temperature parameter) controls the level of exploration. A high $ \\tau $ encourages exploration (more randomness), while a low $ \\tau $ encourages exploitation of the best-known actions.\n",
    "\n",
    "### **Upper Confidence Bound (UCB) Policy** (Exploration approach):\n",
    "  - **Mathematically**:\n",
    "    $$\n",
    "    \\pi_{\\text{UCB}}(a) = \\begin{cases} \n",
    "    1, & \\text{if } a = \\arg\\max_{b} \\left[ Q(b) + c \\cdot \\sqrt{\\frac{\\ln t}{n(b)}} \\right] \\\\\n",
    "    0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    Here, $ n(a) $ is the number of times action $ a $ has been taken, $ t $ is the current timestep, and $ c $ is a constant controlling exploration.\n",
    "    \n",
    "     The UCB formula **ensures untried actions are explored more**.\n",
    "\n",
    "  - **Upper Confidence Bound (UCB) Bandit Algorithm**:\n",
    "    - **Input**: Exploration parameter $ c $, maximum timesteps $ T $.\n",
    "    - **Initialization**: $ Q(a) = 0, n(a) = 0 $ for all $ a \\in A $.\n",
    "    - **Steps**:\n",
    "      1. Select action $ a_t $ using the UCB policy.\n",
    "      2. Sample reward $ r_t $.\n",
    "      3. Update $ Q(a_t) $ using the incremental update rule.\n",
    "\n",
    "---\n",
    "### Hyperparameters\n",
    "\n",
    "- **$ \\epsilon $-Greedy**: $ \\epsilon $ controls exploration; higher $ \\epsilon $ increases exploration.\n",
    "- **Optimistic Initialization**: The **initial value** encourages exploration by setting high initial rewards.\n",
    "- **UCB**: The **$ c $** parameter scales exploration; higher $ c $ increases exploration.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Contextual Bandit**\n",
    "\n",
    "- Often the reward distribution of the bandit you face depends on context\n",
    "\n",
    "- When the state also changes based on our action we call it a\n",
    "Markov Decision Process (MDP)\n",
    "\n",
    "- MDPs are the foundation of reinforcement learning, where the goal is to balance exploration and exploitation in dynamic environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
