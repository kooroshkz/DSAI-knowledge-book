{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb414e92",
   "metadata": {},
   "source": [
    "- **Neuron**: Dendriets $\\to$ Soma $\\to$ Axon $\\to$ *xon Terminals $\\to$ Synapse\n",
    "- **McCulloch-Pitts Model** :‌ $ O = sgn( \\sum w_i x_i ) $\n",
    "- **Excitation** a positive effect like positive weights increasing the chance to fire.\n",
    "- **HN** store approximately **0.15N** patterns, where $N$ is the number of neurons.\n",
    "- **Improve Capacity**: **Sparse Coding**: Store patterns with less active neurons, improving efficiency. **Modified Learning Rules**: Use organizing **Storkey Learning Rule** to improve the separation between stored patterns. **Increase the Number of Neurons**: More neurons directly increase the capacity to store patterns. **Hierarchical Networks**: Structuring the memory storage, make seperated layers and groups of network to enhances capacity.\n",
    "- **HN Model formula**: $O_i (t+1) = \\text{sgn} \\left( \\sum_j w_{ij} O_j(t) + b_i \\right)$, where $O_i$ is the output of neuron $i$, $w_{ij}$ is the weight from neuron $j$ to neuron $i$, and $b_i$ is the bias for neuron $i$. $g()$ can be tanh or sigmoid function.\n",
    "- **Weights**: $w_{ij} = \\frac{1}{N}\\xi_i \\xi_j $ (Also shows stability) **Hebbian (Superposition) Form**: $w_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^p \\xi_i^\\mu \\xi_j^\\mu$, where $\\xi_i^\\mu$ is the $i$-th component of the $\\mu$-th stored pattern.\n",
    "- **Update Function**: $O_i (t+1) = \\text{sgn} \\left( \\sum_j w_{ij} O_j(t) \\right)$.\n",
    "- **Stability**: A stored pattern $\\vec{\\xi}$ is **stable** if, when updated, it remains unchanged: $ O_i = \\text{sgn}\\left(\\sum_j w_{ij} \\xi_j \\right) = \\xi_i $\n",
    "- **Attractors**: Patterns that attract nearby states under the update dynamics are called **attractors**. If starting from an input close to $\\vec{\\xi}$, the network converges to $\\vec{\\xi}$: $ O_i(t+1) = \\text{sgn}\\left(\\sum_j w_{ij} O_j(t)\\right) \\to \\vec{\\xi} $ * All stable patterns are attractors, but attractors can also be **spurious** (unwanted).\n",
    "    - **$h_i^\\nu = \\sum_{j=1}^N w_{ij} \\xi_j^\\nu$**: Total input that neuron $i$ receives when the network is showing pattern $v$.\n",
    "    - **Crosstalk term**: $h_i^\\nu$ expansion by Hebbian rule is $h_i^\\nu = \\xi_i^\\nu + A$ where $A$ is the interference caused by all the other stored patterns that are not $v$.\n",
    "        - $A = \\frac{1}{N} \\sum_{\\mu \\neq \\nu} \\sum_{j=1}^N \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu$\n",
    "- **Energy Behavior**: Network tend to have low / stable energy, when new input comes, by updates it tries to minimize the energy function, to reach a stable state.\n",
    "    - **Energy Function**: $E(\\vec{x}) = -\\frac{1}{2} \\vec{x}^T \\mathbf{W} \\vec{x} - \\vec{x}^T \\vec{b}$ \n",
    "     $H = -\\frac{1}{2} \\sum_{i,j} w_{ij} x_i x_j = -\\frac{1}{2} \\mathbf{x}^\\top W \\mathbf{x}$\n",
    "- **Hebbian Rule**: Networks which fire together, wire together\" $w_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^p \\xi_i^\\mu \\xi_j^\\mu$ (Generalized Hebbian Rule)\n",
    "    - The weight between two neurons $i$ and $j$ is strengthened if they are both active (have the same sign) in the stored patterns $\\xi^\\mu$.\n",
    "- **Spurious attractors**: Wrong memories made by mixing stored patterns. They happen when patterns are too similar or too many.\n",
    "    - $\\vec{\\xi}^\\mu \\cdot \\vec{\\xi}^\\nu = 0$ means orthogonal patterns, no spurious attractors, else with bigger $A$ the more spurious attractors.\n",
    "- **$C_i^\\nu = -\\xi_i^\\nu A = 1 - \\xi_i^\\nu h_i^\\nu$** ($\\xi_i^\\nu$ is the true pattern bit for neuron $i$ in pattern $\\nu$. \n",
    "* $h_i^\\nu$ is the total input to neuron $i$ when pattern $\\nu$ is active.)\n",
    "- $ 1 - \\xi_i^\\nu h_i^\\nu > 1 \\implies \\xi_i^\\nu h_i^\\nu < 0$ $\\to$ $C_i^\\nu \\leq 1$, then $\\xi_i^\\nu h_i^\\nu \\geq 0$ **unstable**\n",
    "- **Probability of error**: $P_{error} = \\text{Probability that } C_i^\\nu > 1$, Optimal $P_{error} < 0.01$. **Calculate variance:** $ \\sigma^2 = \\frac{p}{N}$ **Error probability** $P_{error}$ using Gaussian tail: $ P_{error} = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_1^\\infty e^{-\\frac{x^2}{2\\sigma^2}} dx $ **Choose max error** $P_{error}$ and find corresponding $\\frac{p_{max}}{N}$ from tables or calculation. **Calculate max patterns:** $ p_{max} = \\left(\\frac{p_{max}}{N}\\right) \\times N $\n",
    "---\n",
    "- **Maxmin problem** is to find the optimal weight vector.  To maximize the margin, you try to maximize the minimum distance (margin).\n",
    "- $y^p \\text{(Expected output)} \\cdot f(x) \\text{(Expected output)} $\n",
    "    - if **positive**: correct classification\n",
    "    - if $y \\cdot f(x) \\geq \\gamma \\text{margin}$ we are confident.\n",
    "- **Transformation**: Map $ y_1 = \\text{sign}(g_1(\\mathbf{x})), \\quad y_2 = \\text{sign}(g_2(\\mathbf{x})) $\n",
    "---\n",
    "- **Margin and maximin problem**\n",
    "    - **Margin**: Distance from decision boundary $M(\\mathbf{w}) = \\frac{1}{\\|\\mathbf{w}\\|} \\min_{p \\in [1..P]} \\mathbf{w}^T \\hat{\\mathbf{x}}^p$\n",
    "    - **Maximin**: The best $\\mathbf{w}^*$ that maximizes the margin. $ \\mathbf{w}^* = \\arg\\max_{\\mathbf{w}} M(\\mathbf{w}) = \\arg\\max_{\\mathbf{w}} \\min_{p \\in [1..P]} \\frac{\\mathbf{w}^T \\hat{\\mathbf{x}}^p}{\\|\\mathbf{w}\\|}$\n",
    "    - **Perceptron Optimality**: Perceptron to classify points correctly with some margin\n",
    "        - For each training point $p$ with label $y^p$, we want $\\mathbf{w}^T \\hat{\\mathbf{x}}^p > N \\rho$\n",
    "            - $N$: number of input features, $\\rho$: positive number controlling the margin size\n",
    "        - We update when this margin condition is failed:\n",
    "            - $ \\Delta_j = \\eta \\Theta \\big( N \\rho - y_p \\mathbf{w}^T \\mathbf{x}^p \\big) x_j^p y_p$\n",
    "                - $\\Theta$ : **unit step function** (1 if input ≥ 0, else 0)\n",
    "                - $\\eta$: learning rate\n",
    "\n",
    "- **Regression with continuous outputs**:\n",
    "  - **Saturating activation functions:** Output is limited to range like [0,1] or [-1,1] (e.g., sigmoid, tanh) can lead to vanishing gradients, making training slow.\n",
    "  - **Non-saturating activation functions:** Output is zero for y<0 and the same if y>= 0 (e.g., ReLU) help avoid this issue, allowing for faster convergence.\n",
    "- **Multi-label classification**: If there are multiple labels the output.\n",
    "  - **Softmax activation function**: Converts raw scores into probabilities for each class.\n",
    "  - **Cross-entropy loss**: Measures the difference between predicted probabilities and actual labels.\n",
    "- **Gradient Descent** Variants\n",
    "  - **Batch Gradient Descent:**\n",
    "    Uses **all** data points to calculate gradients. Precise but slow and needs more memory.\n",
    "  - **Stochastic Gradient Descent (SGD):**\n",
    "    Uses **one random** data point per update. Faster but noisier.\n",
    "  - **Mini-batch Gradient Descent:**\n",
    "    Uses a small batch (e.g., 32 or 64) of random points. Combines pros of batch and SGD.\n",
    "- **Epoch** is when a dataset entirely passed but **iteration** is one update step which may not all data points passed (in mini-batch everytime a part of the dataset is used)\n",
    "- **Momentum** and **Nesterov Accelerated Gradient**: Techniques to improve convergence speed and stability in gradient descent by smoothing, reducing oscillations and helping escape local minima (by **inertia**)\n",
    "- **Naïve Approach**: Fully Connected Network on Flattened Images (N, 1)\n",
    "- **Vanishing Gradient Problem**: Deeper CNNs can suffer tiny gradients in early layers. $\\to$ Solution: **Residual Networks (ResNet)** $\\to$ ResNet uses **skip connections** (shortcut paths) to allow gradients to flow directly.\n",
    "- **Vanilla Recurrent Neural Network** $O(t) = g(\\vec{W}^T \\vec{x} + w_2 O(t-1) + b)$\n",
    "  - **Hidden state**: $h(t)$ that acts as short-term memory = previous hidden state $h(t-1)$ + current input $x(t)$.\n",
    "  - **Output** $O(t)$ depends on input $x(t)$ and previous hidden state $h(t-1)$.\n",
    "    - $O(t) = g(W_h h(t-1) + W_x x(t))$, $h(t) = sigmoid([[x(t)],[h(t-1)]])$,  $O(t) = g(h(t))$\n",
    "    - **Backpropagation Through Time (BPTT)**: Unfold the RNN over time steps, compute gradients, and update weights.\n",
    "- The one with $f_t$ is **Forget Gate LSTM**., The one with $i_t$ and $\\tilde{C}(t)$ is **Input Gate LSTM**. The one with $o_t$ is **Output Gate LSTM**. The one in top is **Cell State Update**\n",
    "- **$C_t$/Constant Error Carousel (CEC)**: contains the cell state value at time $t$ ensures gradients don't decay, as it allows for old histories to be forwarded to current time step. \n",
    "- **$\\tilde{C}(t)$**: The candidate cell state with new information to be added.\n",
    "- **$i_t$**: The input gate controlling how much new candidate info (Ĉ\\_t) is added to the cell state.\n",
    "- **Sequence-to-Sequence (Seq2seq) Models**: For machine translation and multi-step forecasting. with two steps:\n",
    "\n",
    "  1. **Encoder**: Encodes the input sequence into a fixed-size context vector. Encodes input sequence into a hidden representation uses word embeddings + RNN/LSTM layers and initial hidden state often random.\n",
    "  2. **Decoder**: Uses the context vector to generate the output sequence. Generates output sequence from the encoder’s final hidden state, using previous outputs as inputs for next steps.\n",
    "---\n",
    "- **Generative Modeling** = Unsupervised task\n",
    "    - **Autoencoders** = Encoder + Decoder $\\to$ Sub-networks of Autoencoders.\n",
    "        - **Encoder** = Compresses input to a lower-dimensional representation. $\\to$ Make it latent\n",
    "            - Can be any neural network architecture. Like Image(CNN), Time Series(LSTM), tabular(MLP).\n",
    "        - **Decoder** = Reconstruct the data from the latent code.\n",
    "            - Another neural network architecture.\n",
    "        - **Deconvolutional Autoencoders**:\n",
    "            - **Upsampling (interpolation)** Adding new pixels between the original ones by guessing their values.\n",
    "            - **Transposed Convolution**: A convolutional layer using filter (kernel) the opposite way.\n",
    "    - **Loss function** or **reconstruction error**. It measures how close the output image $x'$ is to the input image $x$.\n",
    "        - $\\mathcal{L}_{AE}(\\phi, \\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\mathbf{x}^{(i)} - f_{\\theta}\\left(g_{\\phi}(\\mathbf{x}^{(i)})\\right) \\right]^2$\n",
    "        - $id \\approx f_\\theta \\circ g_\\phi$\n",
    "            - $g_\\phi$ is the **encoder** that compresses the input to a smaller representation $z$,\n",
    "            - $f_\\theta$ is the **decoder** that reconstructs the input from $z$.\n",
    "- **Generative Adversarial Network (GAN)** = Unsupervised task\n",
    "    - **Generator**: Generates new data samples.\n",
    "    - **Discriminator**: Classifies real vs. generated data.\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
