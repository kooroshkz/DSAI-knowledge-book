{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **Maxmin problem** is to find the optimal weight vector.  To maximize the margin, you try to maximize the minimum distance (margin).\n",
    "- $y^p \\text{(Expected output)} \\cdot f(x) \\text{(Expected output)} $\n",
    "    - if **positive**: correct classification\n",
    "    - if $y \\cdot f(x) \\geq \\gamma \\text{margin}$ we are confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Common Activation Functions**:\n",
    "  1. **Step function (Heaviside function)**: Outputs either 0 or 1 (binary classification).\n",
    "  2. **Sigmoid function**: Outputs values between 0 and 1. Often used for probabilistic outputs.\n",
    "  3. **Tanh function**: Outputs values between -1 and 1. It is a scaled version of the sigmoid.\n",
    "  4. **ReLU (Rectified Linear Unit)**: Outputs 0 for negative inputs and the input itself for positive inputs. It has become one of the most widely used activation functions for hidden layers.\n",
    "\n",
    "\n",
    "| Function    | Formula                                | Pros & Cons                                                                                  |\n",
    "| ----------- | -------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **tanh**    | $\\frac{e^h - e^{-h}}{e^h + e^{-h}}$    | Saturating, outputs between -1 and 1                                                         |\n",
    "| **ReLU**    | $\\max(0, h)$                           | Efficient, reduces vanishing gradient, sparse activation but can \"die\" (zero output forever) |\n",
    "| **Sigmoid** | $\\sigma(h) = \\frac{1}{1 + e^{-h}}$     | Good for binary classification (probabilities) but saturates                                 |\n",
    "| **Softmax** | $g_i = \\frac{e^{O_i}}{\\sum_j e^{O_j}}$ | Converts vector into probability distribution for multi-class                                |\n",
    "\n",
    "### **Learning Rule (Hebb's rule)**\n",
    "\n",
    "$$\n",
    "O_p = \\text{sign}(w^T x_p)\n",
    "$$\n",
    "\n",
    "* If the output $O_p$ is wrong , update weights:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j + \\Delta_j\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Delta_j = \\begin{cases}\n",
    "2 \\eta y_p x_{jp}, & \\text{if } O_p \\neq y_p \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} = \\eta (1 - O_p y_p) x_{jp} \\quad = \\quad \\Delta_j = \\eta (y_p - O_p) x_{jp}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- If the output $ O_p $ is incorrect, the weights are updated as follows:\n",
    "  $$\n",
    "  w' = w + \\eta y_p x_p\n",
    "  $$\n",
    "  Where:\n",
    "  - $ \\eta $ is the **learning rate** (a small constant, typically between 0 and 1).\n",
    "  - $ y_p $ is the true label (+1 or -1).\n",
    "  - $ x_p $ is the input vector for the current pattern.\n",
    "\n",
    "---\n",
    "\n",
    "- **Linearly separable**: Seperate 1 and 0 data with a straight line through **AND logical function**.\n",
    "\n",
    "- **Non-linearly separable**: Problems like **XOR** (exclusive OR) are not linearly separable $\\to$ MLP will map to higher-dimensional space to be separable.\n",
    "\n",
    "- **Transformation**: We apply two functions, $ g_1(\\mathbf{x}) $ and $ g_2(\\mathbf{x}) $, to map the inputs to a new space.\n",
    "   - We compute:  \n",
    "     $$\n",
    "     y_1 = \\text{sign}(g_1(\\mathbf{x})), \\quad y_2 = \\text{sign}(g_2(\\mathbf{x}))\n",
    "     $$\n",
    "3. **New space**: The transformed data points become **linearly separable** in this new space, meaning a straight line can now divide the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Margin and maximin problem**\n",
    "    - **Margin**: Distance from decision boundary $M(\\mathbf{w}) = \\frac{1}{\\|\\mathbf{w}\\|} \\min_{p \\in [1..P]} \\mathbf{w}^T \\hat{\\mathbf{x}}^p$\n",
    "    - **Maximin**: The best $\\mathbf{w}^*$ that maximizes the margin. $ \\mathbf{w}^* = \\arg\\max_{\\mathbf{w}} M(\\mathbf{w}) = \\arg\\max_{\\mathbf{w}} \\min_{p \\in [1..P]} \\frac{\\mathbf{w}^T \\hat{\\mathbf{x}}^p}{\\|\\mathbf{w}\\|}$\n",
    "    - **Perceptron Optimality**: Perceptron to classify points correctly with some margin\n",
    "        - For each training point $p$ with label $y^p$, we want $\\mathbf{w}^T \\hat{\\mathbf{x}}^p > N \\rho$\n",
    "            - $N$: number of input features, $\\rho$: positive number controlling the margin size\n",
    "        - We update when this margin condition is failed:\n",
    "            - $ \\Delta_j = \\eta \\Theta \\big( N \\rho - y_p \\mathbf{w}^T \\mathbf{x}^p \\big) x_j^p y_p$\n",
    "                - $\\Theta$ : **unit step function** (1 if input â‰¥ 0, else 0)\n",
    "                - $\\eta$: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Proof of Convergence of the Learning Rule\n",
    "\n",
    "Proof if data is linearly separable (there exists some $w^*$ that classifies all correctly with margin > 0), then the perceptron learning algorithm **will find weights $w$ that classify correctly after a finite number of updates**.\n",
    "\n",
    "- Define margin of best solution $M(w^*) = \\frac{1}{\\|w^*\\|} \\min_p w^{*T} \\hat{x}_p > 0$\n",
    "\n",
    "- Starting from $w=0$, after updating on misclassified points: $ w = \\eta \\sum_{p=1}^P H^p \\hat{x}_p $\n",
    "\n",
    "- $ w \\cdot w^* = \\eta \\sum_p H^p \\hat{x}_p \\cdot w^* \\geq \\eta \\min_p (\\hat{x}_p \\cdot w^*) \\sum_p H^p = \\eta H M(w^*) \\|w^*\\| $\n",
    "\n",
    "- The projection $w \\cdot w^*$ grows **at least linearly with $H$**.\n",
    "\n",
    "- At each update: $ \\Delta \\|w\\|^2 = \\|w + \\eta \\hat{x}_p\\|^2 - \\|w\\|^2 = \\eta^2 \\|\\hat{x}_p\\|^2 + 2 \\eta w \\cdot \\hat{x}_p $\n",
    "\n",
    "- Since $\\hat{x}_p$ components are bounded (usually $\\pm 1$), assume: $ \\|\\hat{x}_p\\|^2 \\leq N \\to w \\cdot \\hat{x}_p \\leq N p \\to \\Delta \\|w\\|^2 \\leq N \\eta (\\eta + 2p) $\n",
    "- After $H$ steps: $\\|w\\|^2 \\leq H N \\eta (\\eta + 2p)$, so $\\|w\\|$ grows at most proportional to $\\sqrt{H}$.\n",
    "- Look at normalized overlap: $\\frac{w \\cdot w^*}{\\|w\\|} \\geq \\frac{\\eta H M(w^*) \\|w^*\\|}{\\sqrt{H} \\sqrt{N \\eta (\\eta + 2p)}}$, which cannot grow without bound because cosine between $w$ and $w^*$ is at most 1.\n",
    "- Define $\\phi = \\frac{(w \\cdot w^*)^2}{\\|w\\|^2 \\|w^*\\|^2} = \\cos^2(\\alpha) \\leq 1$. \n",
    "- From the previous inequality, $1 \\geq \\phi \\geq H \\frac{M(w^*)^2 \\eta}{N (\\eta + 2p)}$\n",
    "- Rearranging gives us a bound on $H$: $ H \\leq \\frac{N (\\eta + 2p)}{M(w^*)^2 \\eta} $\n",
    "- This says the total number of updates $H$ is **bounded and finite**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
