{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "- **Common Activation Functions**:\n",
    "  1. **Step function (Heaviside function)**: Outputs either 0 or 1 (binary classification).\n",
    "  2. **Sigmoid function**: Outputs values between 0 and 1. Often used for probabilistic outputs.\n",
    "  3. **Tanh function**: Outputs values between -1 and 1. It is a scaled version of the sigmoid.\n",
    "  4. **ReLU (Rectified Linear Unit)**: Outputs 0 for negative inputs and the input itself for positive inputs. It has become one of the most widely used activation functions for hidden layers.\n",
    "\n",
    "![Activation Functions](https://www.researchgate.net/profile/Aaron-Stebner-2/publication/341310767/figure/fig7/AS:890211844255749@1589254451431/Common-activation-functions-in-artificial-neural-networks-NNs-that-introduce.ppm)\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Rule**\n",
    "\n",
    "$$\n",
    "O_p = \\text{sign}(w^T x_p)\n",
    "$$\n",
    "\n",
    "* If the output $O_p$ is wrong , update weights:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j + \\Delta_j\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Delta_j = \\begin{cases}\n",
    "2 \\eta y_p x_{jp}, & \\text{if } O_p \\neq y_p \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} = \\eta (1 - O_p y_p) x_{jp} \\quad = \\quad \\Delta_j = \\eta (y_p - O_p) x_{jp}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- If the output $ O_p $ is incorrect, the weights are updated as follows:\n",
    "  $$\n",
    "  w' = w + \\eta y_p x_p\n",
    "  $$\n",
    "  Where:\n",
    "  - $ \\eta $ is the **learning rate** (a small constant, typically between 0 and 1).\n",
    "  - $ y_p $ is the true label (+1 or -1).\n",
    "  - $ x_p $ is the input vector for the current pattern.\n",
    "\n",
    "---\n",
    "\n",
    "The **AND logical function** is **linearly separable** because you can draw a straight line that separates the true outputs from the false ones.\n",
    "\n",
    "- **Non-linearly separable**: Problems like **XOR** (exclusive OR) are not linearly separable, meaning no single line can separate the two classes.\n",
    "\n",
    "#### **XOR Table**:\n",
    "\n",
    "| Input  | Output |\n",
    "|--------|--------|\n",
    "| (0,0)  |   0    |\n",
    "| (0,1)  |   1    |\n",
    "| (1,0)  |   1    |\n",
    "| (1,1)  |   0    |\n",
    "\n",
    "#### **Solution**:  \n",
    "- You can use a **multi-layer network** (e.g., a multilayer perceptron with a hidden layer) to transform the data into a higher-dimensional space where it becomes linearly separable.\n",
    "- **Transformation**: We apply two functions, $ g_1(\\mathbf{x}) $ and $ g_2(\\mathbf{x}) $, to map the inputs to a new space.\n",
    "   - We compute:  \n",
    "     $$\n",
    "     y_1 = \\text{sign}(g_1(\\mathbf{x})), \\quad y_2 = \\text{sign}(g_2(\\mathbf{x}))\n",
    "     $$\n",
    "3. **New space**: The transformed data points become **linearly separable** in this new space, meaning a straight line can now divide the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Margin and maximin problem**\n",
    "    - **Margin**: Distance from decision boundary $M(\\mathbf{w}) = \\frac{1}{\\|\\mathbf{w}\\|} \\min_{p \\in [1..P]} \\mathbf{w}^T \\hat{\\mathbf{x}}^p$\n",
    "    - **Maximin**: The best $\\mathbf{w}^*$ that maximizes the margin. $ \\mathbf{w}^* = \\arg\\max_{\\mathbf{w}} M(\\mathbf{w}) = \\arg\\max_{\\mathbf{w}} \\min_{p \\in [1..P]} \\frac{\\mathbf{w}^T \\hat{\\mathbf{x}}^p}{\\|\\mathbf{w}\\|}$\n",
    "    - **Perceptron Optimality**: Perceptron to classify points correctly with some margin\n",
    "        - For each training point $p$ with label $y^p$, we want $\\mathbf{w}^T \\hat{\\mathbf{x}}^p > N \\rho$\n",
    "            - $N$: number of input features, $\\rho$: positive number controlling the margin size\n",
    "        - We update when this margin condition is failed:\n",
    "            - $ \\Delta_j = \\eta \\Theta \\big( N \\rho - y_p \\mathbf{w}^T \\mathbf{x}^p \\big) x_j^p y_p$\n",
    "                - $\\Theta$ : **unit step function** (1 if input â‰¥ 0, else 0)\n",
    "                - $\\eta$: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Proof of Convergence of the Learning Rule\n",
    "\n",
    "Proof if data is linearly separable (there exists some $w^*$ that classifies all correctly with margin > 0), then the perceptron learning algorithm **will find weights $w$ that classify correctly after a finite number of updates**.\n",
    "\n",
    "---\n",
    "\n",
    "- Define margin of best solution $M(w^*) = \\frac{1}{\\|w^*\\|} \\min_p w^{*T} \\hat{x}_p > 0$\n",
    "\n",
    "- Starting from $w=0$, after updating on misclassified points: $ w = \\eta \\sum_{p=1}^P H^p \\hat{x}_p $\n",
    "\n",
    "- $ w \\cdot w^* = \\eta \\sum_p H^p \\hat{x}_p \\cdot w^* \\geq \\eta \\min_p (\\hat{x}_p \\cdot w^*) \\sum_p H^p = \\eta H M(w^*) \\|w^*\\| $\n",
    "\n",
    "- The projection $w \\cdot w^*$ grows **at least linearly with $H$**.\n",
    "\n",
    "- At each update: $ \\Delta \\|w\\|^2 = \\|w + \\eta \\hat{x}_p\\|^2 - \\|w\\|^2 = \\eta^2 \\|\\hat{x}_p\\|^2 + 2 \\eta w \\cdot \\hat{x}_p $\n",
    "\n",
    "- Since $\\hat{x}_p$ components are bounded (usually $\\pm 1$), assume: $ \\|\\hat{x}_p\\|^2 \\leq N \\to w \\cdot \\hat{x}_p \\leq N p \\to \\Delta \\|w\\|^2 \\leq N \\eta (\\eta + 2p) $\n",
    "- After $H$ steps: $\\|w\\|^2 \\leq H N \\eta (\\eta + 2p)$, so $\\|w\\|$ grows at most proportional to $\\sqrt{H}$.\n",
    "- Look at normalized overlap: $\\frac{w \\cdot w^*}{\\|w\\|} \\geq \\frac{\\eta H M(w^*) \\|w^*\\|}{\\sqrt{H} \\sqrt{N \\eta (\\eta + 2p)}}$, which cannot grow without bound because cosine between $w$ and $w^*$ is at most 1.\n",
    "- Define $\\phi = \\frac{(w \\cdot w^*)^2}{\\|w\\|^2 \\|w^*\\|^2} = \\cos^2(\\alpha) \\leq 1$. \n",
    "- From the previous inequality, $1 \\geq \\phi \\geq H \\frac{M(w^*)^2 \\eta}{N (\\eta + 2p)}$\n",
    "- Rearranging gives us a bound on $H$: $ H \\leq \\frac{N (\\eta + 2p)}{M(w^*)^2 \\eta} $\n",
    "- This says the total number of updates $H$ is **bounded and finite**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
