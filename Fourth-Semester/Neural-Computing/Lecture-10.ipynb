{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a631544",
   "metadata": {},
   "source": [
    "- **Vanilla Recurrent Neural Network**\n",
    "  - **Hidden state**: $h(t)$ that acts as short-term memory = previous hidden state $h(t-1)$ + current input $x(t)$.\n",
    "  - **Output** $O(t)$ depends on input $x(t)$ and previous hidden state $h(t-1)$.\n",
    "    - $O(t) = g(W_h h(t-1) + W_x x(t))$\n",
    "    - $h(t) = sigmoid(\\begin{bmatrix}x(t) \\\\h(t-1)\\end{bmatrix})$\n",
    "    - $O(t) = g(h(t))$\n",
    "    - **Backpropagation Through Time (BPTT)**: Unfold the RNN over time steps, compute gradients, and update weights.\n",
    "  - **Problems**: \n",
    "    - **Vanishing gradients**: Gradients become too small, causing the network to forget early inputs.\n",
    "    - **Exploding gradients**: Gradients become too large, leading to unstable training.\n",
    "  - **Solutions**:\n",
    "    - **Constant Error Carousel**: A mechanism to help gradients flow through many time steps by having long memory $C_t$.\n",
    "- **Gated Recurrent Units (GRUs)**: A simpler alternative to LSTMs with fewer gates.\n",
    "- **Long Short-Term Memory (LSTM) Networks** (Gated RNNs)\n",
    "  - **Gates** control information flow:\n",
    "    - **Forget gate $f_t$**: Decides what to discard from the cell state.\n",
    "    - **Input gate $i_t$**: Decides what new information to add.\n",
    "    - **Output gate $o_t$**: Decides what part of the cell state to output.\n",
    "  - **Notations**:\n",
    "    - $C_t$: Cell state carrying long-term memory.\n",
    "    - $h_t$: Hidden state at time $t$.\n",
    "    - **$f(t), i(t), o(t)$**: Sigmoid gates controlling information flow.\n",
    "    - **$\\tilde{C}_t$**: Candidate cell state, computed using tanh to keep values in a suitable range.\n",
    "  - **Flow**:\n",
    "    - **Inputs**: $C_{t-1}, h_{t-1}, x_t$\n",
    "    - $h_{t-1}$ and $x_t$ passes through sigmoid to produce  $f_t, i_t, o_t$.\n",
    "      - $\\vec{C}(t) = f(t) * \\vec{C}(t - 1) + \\text{Input}(t)$\n",
    "      - $f(t) = \\sigma \\left( \\mathbf{W}_f \\left[ \\vec{h}(t - 1), \\vec{x}(t) \\right] + b_f \\right)$\n",
    "    - $h_{t-1}$ and $x_t$ passes through sigmoid to produce $i_t$ and $tanh$ to produce $\\tilde{C}_t$.\n",
    "      - $\\text{Input}(t) = i(t) * \\tilde{C}(t)$\n",
    "      - $i(t) = \\sigma \\left( \\mathbf{W}_i \\left[ \\vec{h}(t - 1), \\vec{x}(t) \\right] + b_i \\right)$\n",
    "      - $\\tilde{C}(t) = \\tanh \\left( \\mathbf{W}_C \\left[ \\vec{h}(t - 1), \\vec{x}(t) \\right] + b_C \\right)$\n",
    "    - Now $C_t$ formed and $h_t$ can be computed. We pass $h_{t-1}$ and $x_t$ through sigmoid to produce $o_t$.\n",
    "      - $o(t) = \\sigma \\left( \\mathbf{W}_o \\left[ \\vec{h}(t - 1), \\vec{x}(t) \\right] + b_o \\right)$\n",
    "      - $\\vec{h}(t) = o(t) * \\tanh(\\vec{C}(t))$\n",
    "      \n",
    "<img src=\"../../Files/fourth-semester/nc/1.png\" style=\"width:550px;\">\n",
    "\n",
    "- **Sequence-to-Sequence (Seq2seq) Models**: For machine translation and multi-step forecasting. with two steps:\n",
    "\n",
    "  1. **Encoder**: Encodes the input sequence into a fixed-size context vector. Encodes input sequence into a hidden representation uses word embeddings + RNN/LSTM layers and initial hidden state often random.\n",
    "  2. **Decoder**: Uses the context vector to generate the output sequence. Generates output sequence from the encoderâ€™s final hidden state, using previous outputs as inputs for next steps.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
