{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **The Hopfield Network (HN)**\n",
    "- **Fully connected** Network with **weight matrix**\n",
    "- Neurons have two possible states: **-1** (inactive) or **+1** (active)., which is a thresholded output.\n",
    "- **Bidirectional connections** between neurons.\n",
    "- The network stores a set of patterns $\\xi_1, \\xi_2, \\dots, \\xi_p$ helps in pattern recognition and memory recall.\n",
    "- The network can store approximately **0.15N** patterns, where $N$ is the number of neurons.\n",
    "- **Improve Capacity**:\n",
    "    - **Sparse Coding**: Store patterns with less active neurons, improving efficiency.\n",
    "    - **Modified Learning Rules**: Use organizing **Storkey Learning Rule** to improve the separation between stored patterns.\n",
    "    - **Increase the Number of Neurons**: More neurons directly increase the capacity to store patterns.\n",
    "    - **Hierarchical Networks**: Structuring the memory storage, make seperated layers and groups of network to enhances capacity.\n",
    "- **HN Model formula**:\n",
    "    - $O_i (t+1) = \\text{sgn} \\left( \\sum_j w_{ij} O_j(t) + b_i \\right)$, where $O_i$ is the output of neuron $i$, $w_{ij}$ is the weight from neuron $j$ to neuron $i$, and $b_i$ is the bias for neuron $i$. $g()$ can be tanh or sigmoid function.\n",
    "- **Weights**: The weight is defined as $w_{ij} = \\frac{1}{N}\\xi_i \\xi_j $ (Also shows stability)\n",
    "    - **Hebbian (Superposition) Form**: $w_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^p \\xi_i^\\mu \\xi_j^\\mu$, where $\\xi_i^\\mu$ is the $i$-th component of the $\\mu$-th stored pattern.\n",
    "    - The weights are symmetric, i.e., $w_{ij} = w_{ji}$, and $w_{ii} = 0$ (no self-connections).\n",
    "- **Update Function**: Input pattern $\\to$ update states (1 at the time) $\\to$ get closer to one of patterns. \n",
    "    - $O_i (t+1) = \\text{sgn} \\left( \\sum_j w_{ij} O_j(t) \\right)$.\n",
    "- **Stability**: A stored pattern $\\vec{\\xi}$ is **stable** if, when updated, it remains unchanged: $ O_i = \\text{sgn}\\left(\\sum_j w_{ij} \\xi_j \\right) = \\xi_i $\n",
    "- **Attractors**: Patterns that attract nearby states under the update dynamics are called **attractors**. If starting from an input close to $\\vec{\\xi}$, the network converges to $\\vec{\\xi}$: $ O_i(t+1) = \\text{sgn}\\left(\\sum_j w_{ij} O_j(t)\\right) \\to \\vec{\\xi} $ * All stable patterns are attractors, but attractors can also be **spurious** (unwanted).\n",
    "    - **$h_i^\\nu = \\sum_{j=1}^N w_{ij} \\xi_j^\\nu$**: Total input that neuron $i$ receives when the network is showing pattern $v$.\n",
    "    - **Crosstalk term**: $h_i^\\nu$ expansion by Hebbian rule is $h_i^\\nu = \\xi_i^\\nu + A$ where $A$ is the interference caused by all the other stored patterns that are not $v$.\n",
    "        - $A = \\frac{1}{N} \\sum_{\\mu \\neq \\nu} \\sum_{j=1}^N \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu$\n",
    "- **Energy Behavior**: Network tend to have low / stable energy, when new input comes, by updates it tries to minimize the energy function, to reach a stable state.\n",
    "    - **Energy Function**: $E(\\vec{x}) = -\\frac{1}{2} \\vec{x}^T \\mathbf{W} \\vec{x} - \\vec{x}^T \\vec{b}$\n",
    "    - $H = -\\frac{1}{2} \\sum_{i,j} w_{ij} x_i x_j = -\\frac{1}{2} \\mathbf{x}^\\top W \\mathbf{x}$\n",
    "- **Hamming Distance**: The goal is to minimize the Hamming distance between the input pattern and the stored patterns.\n",
    "    - $ d(\\vec{x}, \\vec{\\xi}) = \\sum_{i=1}^N [ \\xi_i (1 - x_i) + (1 - \\xi_i) x_i ] = $ no mismatch between patterns \n",
    "    - Instead binary state $\\begin{pmatrix} 1 \\\\  0 \\end{pmatrix} $ we use $\\text{FIRING} = \\text{unit value} +1$ and $\\text{Inhibit (NOT FIRING)} = \\text{unit value} -1$.\n",
    "- **Hebbian Rule**: Networks which fire together, wire together\" $w_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^p \\xi_i^\\mu \\xi_j^\\mu$ (Generalized Hebbian Rule)\n",
    "    - The weight between two neurons $i$ and $j$ is strengthened if they are both active (have the same sign) in the stored patterns $\\xi^\\mu$.\n",
    "- **Spurious attractors**: Wrong memories made by mixing stored patterns. They happen when patterns are too similar or too many.\n",
    "    - $\\vec{\\xi}^\\mu \\cdot \\vec{\\xi}^\\nu = 0$ means orthogonal patterns, no spurious attractors, else with bigger $A$ the more spurious attractors.\n",
    "- **$C_i^\\nu = -\\xi_i^\\nu A = 1 - \\xi_i^\\nu h_i^\\nu$** ($\\xi_i^\\nu$ is the true pattern bit for neuron $i$ in pattern $\\nu$. * $h_i^\\nu$ is the total input to neuron $i$ when pattern $\\nu$ is active.)\n",
    "- $ 1 - \\xi_i^\\nu h_i^\\nu > 1 \\implies \\xi_i^\\nu h_i^\\nu < 0$ $\\to$ $C_i^\\nu \\leq 1$, then $\\xi_i^\\nu h_i^\\nu \\geq 0$ **unstable**\n",
    "- **Probability of error**: $P_{error} = \\text{Probability that } C_i^\\nu > 1$**, Optimal $P_{error} < 0.01$.\n",
    "- **Calculate variance:** $ \\sigma^2 = \\frac{p}{N}$\n",
    "- **Error probability** $P_{error}$ using Gaussian tail: $ P_{error} = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_1^\\infty e^{-\\frac{x^2}{2\\sigma^2}} dx $\n",
    "- **Choose max error** $P_{error}$ and find corresponding $\\frac{p_{max}}{N}$ from tables or calculation.\n",
    "- **Calculate max patterns:** $ p_{max} = \\left(\\frac{p_{max}}{N}\\right) \\times N $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Proves and Exercises**:\n",
    "\n",
    "#### Stability Condition\n",
    "\n",
    "**Definition.** Let’s consider one stored pattern $\\vec{\\xi}$. The condition for $\\vec{\\xi}$ to be a stable pattern (attractor) is\n",
    "\n",
    "$$\n",
    "O_i = \\text{sgn} \\left[ \\sum_{j=1}^N w_{ij} \\xi_j \\right] = \\xi_i \\tag{5}\n",
    "$$\n",
    "\n",
    "**Statement.** We obtain this if $w_{ij} = w_{ji} = \\alpha \\xi_i \\xi_j$, $\\alpha > 0 \\quad $($w_{ij} \\propto \\xi_i \\xi_j$).\n",
    "\n",
    "**Proof.** Let $W = \\alpha \\xi \\xi^T$ be the weight matrix:\n",
    "\n",
    "$$\n",
    "O_i = \\text{sgn} \\left[ \\sum_j w_{ij} \\xi_j \\right] = \\text{sgn} \\left[ \\sum_j \\propto \\xi_i \\xi_j \\xi_j \\right] = \\text{sgn} \\left[ \\alpha \\xi_i \\sum_{j=1}^N 1 \\right] = \\text{sgn}(\\alpha N \\xi_i) = \\text{sgn}(\\xi_i) = \\xi_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow O_i = \\xi_i \\text{ STABLE!} \\Rightarrow \\text{We can take } \\alpha = \\frac{1}{N} \\text{ when } N = \\# \\text{nodes} \\Rightarrow w_{ij} = \\frac{1}{N} \\xi_i \\xi_j\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Design a network of 3 neurons that resembles a pattern $(1, 1, -1)$:\n",
    "\n",
    "$$\n",
    "W = \\frac{1}{N} \\vec{\\xi} \\vec{\\xi}^T = \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} (1, 1, -1) = \\frac{1}{3} \\begin{bmatrix} 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ -1 & -1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "1. Is $\\vec{\\xi}$ stable?\n",
    "\n",
    "$$\n",
    "\\vec{0} = \\text{sgn}(W \\cdot \\vec{\\xi}) = \\text{sgn} \\left[ \\frac{1}{3} \\begin{bmatrix} 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \\right] = \\text{sgn} \\left[ \\frac{1}{3} \\begin{pmatrix} 3 \\\\ 3 \\\\ -3 \\end{pmatrix} \\right] = \\text{sgn} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\vec{\\xi}\n",
    "\\Rightarrow \\text{the answer is YES!}\n",
    "$$\n",
    "\n",
    "\n",
    "2. Is $\\vec{\\xi}$ an attractor? Let’s try a test pattern $\\vec{x} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "By help of the update function $O_i (t+1) = \\text{sgn} \\left( \\sum_j w_{ij} O_j(t) \\right)$ we can calculate the output:\n",
    "\n",
    "$$\n",
    "\\vec{O} = \\text{sgn} \\left[ \\frac{1}{3} \\begin{bmatrix} 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ -1 & -1 & 1 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right] = \\text{sgn} \\left[ \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \\right] = \\text{sgn} \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ -1/3 \\end{pmatrix} = \\vec{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow \\vec{\\xi} = \\text{attractor state}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Statement.** $\\vec{\\xi}$ is an attractor of the network defined by the weights $w_{ij} = \\frac{1}{N} \\xi_i \\xi_j$.\n",
    "\n",
    "**Proof.** Let’s consider the total net input:\n",
    "\n",
    "$$\n",
    "h_i = \\sum_{j=1}^N w_{ij} x_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow h_i = \\sum_j \\frac{1}{N} \\xi_i \\xi_j x_j = \\frac{1}{N} \\xi_i \\sum_j \\xi_j x_j = \\frac{1}{N} \\xi_i \\left[ \\sum_{j=\\text{correct}} \\xi_j x_j + \\sum_{j=\\text{wrong}} \\xi_j x_j \\right] = \\frac{\\xi_i}{N} (N_{\\text{correct}} - N_{\\text{wrong}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow O_i = \\text{sgn}(h_i) = \\text{sgn} \\left( \\xi_i \\left[ \\frac{N_{\\text{correct}} - N_{\\text{wrong}}}{N} \\right] \\right) = \\begin{cases} \\xi_i & \\text{if more than or exactly } \\frac{N}{2} \\text{ are correct} \\\\ -\\xi_i & \\text{otherwise/opposite case} \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\xi_i \\text{ is an attractor! AND in this simple case we also have another attractor at the reversed state } -\\xi.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Stability Condition**\n",
    "\n",
    "The **Stability Condition** for a pattern $\\xi_i^\\nu$ is $O_i = \\mathrm{sgn}(h_i^\\nu) = \\xi_i^\\nu, \\quad \\forall i = 1, \\ldots, N.$\n",
    "\n",
    "**Proof:**\n",
    "Let’s consider the net input\n",
    "\n",
    "$$\n",
    "h_i^\\nu \\equiv \\sum_{j=1}^N w_{ij} \\xi_j^\\nu = \\frac{1}{N} \\sum_{j=1}^N \\sum_{\\mu=1}^p \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\left(\\sum_{j=1}^N \\xi_i^\\nu \\xi_j^\\nu \\xi_j^\\nu \\right) + \\sum_{j=1}^N \\sum_{\\mu \\neq \\nu} \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{j=1}^N \\xi_i^\\nu + \\frac{1}{N} \\sum_{j=1}^N \\sum_{\\mu \\neq \\nu} \\xi_i^\\mu \\xi_j^\\mu \\nu_j^\\nu =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\xi_i^\\nu + \\frac{1}{N} \\sum_{j=1}^N \\sum_{\\mu \\neq \\nu} \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu \\equiv \\xi_i^\\nu + A\n",
    "$$\n",
    "\n",
    "Now, if $|A| < 1$, then $O_i = \\mathrm{sgn}(h_i^\\nu) = \\mathrm{sgn}(\\xi_i^\\nu + A) = \\mathrm{sgn}(\\xi_i^\\nu) = \\xi_i^\\nu \\implies \\mathbf{Stability}.$\n",
    "$A$ is named as *crosstalk* term and refers to the undesired interactions between stored patterns, generating interference, and hence retrieval error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Proof: Energy $H$ decreases with each update\n",
    "\n",
    "- Energy function: $ H = -\\frac{1}{2} \\sum_{ij} w_{ij} x_i x_j$\n",
    "- Split $H$ as: $ H = C - \\sum_{i \\neq j} w_{ij} x_i x_j $ where $C$ is constant.\n",
    "- Update rule for neuron $i$: $ O_i(t+1) = \\text{sgn}\\left(\\sum_j w_{ij} O_j(t)\\right) $\n",
    "- Two cases:\n",
    "   - 1. If $O_i(t+1) = O_i(t)$ (no energy change) $ H(t+1) = H(t)$\n",
    "   - 2. If $O_i(t+1) = -O_i(t)$ (energy decreases). $ Difference: $H(t+1) - H(t) = 2 O_i(t) \\sum_j w_{ij} O_j(t) - 2 w_{ii}$\n",
    "\n",
    "- Since $O_i(t+1)$ flips sign, the weighted sum $\\sum_j w_{ij} O_j(t)$ has opposite sign to $O_i(t)$.\n",
    "- This makes the product $O_i(t) \\sum_j w_{ij} O_j(t)$ negative. Also, $w_{ii}$ is positive or zero.\n",
    "- So, $ H(t+1) - H(t) < 0 $\n",
    "- Energy $H$ **always decreases or stays the same** after an update. Therefore, the network converges to a **local minimum** of $H$. These minima correspond to **stable states** of the network."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
