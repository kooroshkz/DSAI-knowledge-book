{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbae187",
   "metadata": {},
   "source": [
    "- **Key Ideas:**\n",
    "    - Nearby pixels are related\n",
    "    - Same patterns appear everywhere (edges, corners)\n",
    "    - **Sparse connections**: Not fully connected and have **local connectivity**\n",
    "    - **Weight sharing**: Reuse the same wights (Numbers in matrix) of filters (Kernels) across the image\n",
    "        - **Local Connected networks**: Have sparse connections but do not share weights but **CNN** have both\n",
    "        - We have multiple filters to learn different features but each filter has its own weights shared across the image\n",
    "        - **Stride**: How much we move the filter at each step (Usually 1, stride make **downsampling** and skip pixels)\n",
    "            - We can replace by **Pooling** for downsampling\n",
    "        - **Dilated convolution**: We add 0 between filter weights to increase receptive field without pooling which reduces details\n",
    "    - **Pooling**: Downsampling to reduce dimensions and computation\n",
    "    - **Padding**: Without padding image shrinks after each conv, so we often use **zero padding** to keep spatial size\n",
    "\n",
    "### Composing layers\n",
    "- **Convolutional layer**: Apply multiple filters to input to produce feature maps\n",
    "- **Activation function**: Non-linearity like ReLU after conv layer\n",
    "- **Pooling layer**: Downsample feature maps (Max pooling, Average pooling)\n",
    "    - **Max pooling**: Take max value in each region\n",
    "    - **Mean pooling**: Take average value in each region\n",
    "- **Fully connected layer**: At the end to make final predictions\n",
    "\n",
    "If we do only `conv + ReLU` the networks sees local patterns but not global patterns. By **Pooling + downsampling** and stacking **multiple conv** layers, the network can learn hierarchical features from local to global which helps **generalization**.\n",
    "\n",
    "Order is usually: `[Conv -> ReLU -> Downsample] x N -> [FC -> ReLU] x M -> Output Layer`\n",
    "\n",
    "**Dropout** removes random neurons during training to prevent overfitting and other neuron learn better features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5f7b5",
   "metadata": {},
   "source": [
    "\n",
    "1x1 convolutions reduce channels.\n",
    "\n",
    "- **History**:\n",
    "    - **LeNet**: Early CNN for digit recognition on MNIST, with backpropagation, FC, pooling and sigmoid/tanh activations\n",
    "    - **AlexNet**: Deeper CNN with ReLU, dropout, used GPU, trained on ImageNet, pooling, 3 FC 5 Conv -> pool,pool,conv,conv,pool,FC,FC,FC\n",
    "        - **ImageNet**: Large dataset with 1.2M images, 1000 classes\n",
    "    - **VGG**: Very deep CNN with 16-19 layers, small 3x3 filters, more depth improves performance\n",
    "\n",
    "\n",
    "- **Deep Networks Problem**:\n",
    "    - **Vanishing gradients**: Gradients get smaller in deep networks, making training hard\n",
    "    - **Unstable training**\n",
    "    - **Solution**: \n",
    "        - **Weight initialization**:\n",
    "            - Xavier initialization: For tanh/sigmoid activations, keep variance of activations same across layers\n",
    "                - Xavier with ReLU: Kills half the variance and shrink it\n",
    "            - Kaiming (He) initialization: For ReLU activations, keep variance of activations same across layers $\\to$ used in modern CNNs\n",
    "            - Random initialization: For ReLU activations, use He initialization to keep variance\n",
    "        - **Normalization**\n",
    "            - Makes different layers have same train speed\n",
    "            - Stabilizes training\n",
    "            - Let us use higher learning rates\n",
    "            - Force activation to have mean = 0 and variance = 1\n",
    "            - Done during training\n",
    "                - **Batch Normalization**: Normalize across batch dimension, Normalize per feature\n",
    "        - **Residual connections**: Skip connections to help gradients flow better (ResNet) because deeper nets perform worst\n",
    "            - **Basic Block**: 2 conv layers with ReLU and a skip connection\n",
    "            - **Bottleneck Block**: 1x1, 2x2, 3x3 conv layers with skip connection to reduce computation\n",
    "\n",
    "- **Practical details**\n",
    "    - **Rectangular images**: Resize and crop centered\n",
    "    - **Data augmentation**: flipping, cropping, noise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
