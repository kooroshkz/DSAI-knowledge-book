{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b82c67",
   "metadata": {},
   "source": [
    "-**Model Performance**: Model architecture, Loss function, Optimizer, Training data\n",
    "- **Data Collection**:\n",
    "    - **Hand-curated datasets**: researchers manually collect images in small controlled conditions, like COREL, Caltech, PASCAL \n",
    "        - **Pros**: high-quality labels, clean images, carefully chosen \n",
    "        - **Cons**: strong selection bias, narrow worldview, unrealistic examples, lab conditions, same camera, lighting, pose Models trained here learn idealized reality\n",
    "    - **Web + crowdsourcing**: scrape images from search engines, annotate with Mechanical Turk, pay people small amounts Different annotation motivations: money, fun (games), access (CAPTCHAs), curiosity \n",
    "        - **Pros**: much larger datasets, more diversity, scalable \n",
    "        - **Cons**: noisy labels, cultural bias, annotator disagreement, instructions shape labels, search queries inject bias \n",
    "        - **Important experiment**: Humans have biases, don’t always follow instructions, disagree even on simple tasks Human labels are not neutral\n",
    "    - **Synthetic / virtual data**: used in robotics, navigation, action recognition \n",
    "        - **Pros**: perfect labels, infinite data, full control \n",
    "        - **Cons**: domain gap, unrealistic textures, bad human behavior simulation \n",
    "        - **This leads to**: model works in simulation fails in real world\n",
    "    - **Internet-scale weak labels**: examples: CLIP, LAION How: scrape images + alt-text, minimal filtering, no manual labels \n",
    "        - **Properties**: billions of samples, extremely noisy, captions incomplete or wrong, harmful content possible, many datasets not public Labels are now weak signals, not ground\n",
    "\n",
    "\n",
    "- **Dataset bias**:\n",
    "    - **Representation bias**: Some groups appear less often: elderly faces, rural scenes, non-Western contexts **Result**: worse performance on underrepresented groups\n",
    "    - **Sampling bias**: Flickr → Western, wealthy users, Instagram → young demographics **Result**: model learns *who posts*, not *who exists*\n",
    "    - **Measurement bias**: differences in camera type, resolution, viewpoint, lighting **E.g**: training on phone images, testing on CCTV\n",
    "    - **Label / annotation bias**: humans bring assumptions: “cup” vs “mug”, ambiguous categories, cultural differences\n",
    "    - **Context / co-occurrence bias**: objects always appear in same context so model learns background instead of object\n",
    "    - **Temporal bias**: datasets age: cars change, fashion changes, cities change, old datasets = outdated world\n",
    "\n",
    "- **Model shortcuts learning**: a feature correlated with the label but not truly meaningful used to improves training accuracy like background, texture, watermarks, camera artifacts. This is **not a model flaw** — it’s a **data problem**.\n",
    "- **Adversarial noise**: tiny, structured, not random. Works: model relies on non-robust features and gradient shows which pixels matter\n",
    "- **Distribution shift** solutions:\n",
    "    - **Collect better data**: expensive, slow, often impossible, train on data that matches test data, Problems: expensive, slow, often impossible\n",
    "    - **Data augmentation / domain randomization**: add noise, color shifts, blur, geometry changes to make training data more diverse Helps but not enough\n",
    "    - **Domain adaptation via style transfer**: use GANs/CycleGAN to make source data look like target data Works well for visual style but fails for semantic changes, geometry, object presence\n",
    "    - **Domain adaptation in feature space**: align feature distributions by training domain classifier and forcing feature extractor to confuse it Goal: domain-invariant features\n",
    "- **Adaptation** Best to worst: 1. representative data 2. adaptation 3. augmentation\n",
    "- **Societal impact & ethics**: models do what data tells them Bad data → harmful systems like racist chatbots, biased face recognition, wrongful arrests, privacy violations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
