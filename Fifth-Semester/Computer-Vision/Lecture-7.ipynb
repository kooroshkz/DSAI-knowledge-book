{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7a03a7",
   "metadata": {},
   "source": [
    "#### Stochastic (Random) Generation\n",
    "\n",
    "In Image classification we have inputs and one output (label) (Non-stochastic) but in Image generation we have only one input but many possible outputs.\n",
    "\n",
    "A solution like stochastic generation is needed to generate different outputs for the same input.\n",
    "\n",
    "**Conditional** models are used to generate images based on some conditions like class labels or text descriptions, **unconditional** models generate images without any specific conditions so we add conditions for creativity.\n",
    "\n",
    "- **Generative models**\n",
    "    - **Direct approach**: $G: z \\rightarrow x$ where $z$ is random noise (GANs, Diffusion models)\n",
    "    - **Indirect approach**: $E: x \\rightarrow \\R$ where we score generated images and find higher score image\n",
    "- **Autoregressive models**: Generate an image pixel by pixel where predict next pixel **distribution** based on previous pixels and sample \n",
    "        - **Without DL**: Copy patch in image and paste it in another place, make texture, doesn't work on complex images\n",
    "        - **With DL**: Train a next-pixel classifier\n",
    "- **Generative Adversarial Networks (GANs)**: Two networks (Generator and Discriminator) compete where the generator tries to create realistic images and the discriminator tries to distinguish between real and fake images.\n",
    "    - **Generator (G)**: Takes random noise as input and generates images train to fool the discriminator.\n",
    "    - **Discriminator (D)**: Takes an image as input and outputs a probability indicating whether the image is real or fake, trained to classify correctly.\n",
    "    - **Issues**: unstable training, mode collapse (lack of diversity), hard to control generation -> (Conditional GANs).\n",
    "    - **Conditional GANs**: Both G and D receive additional information\n",
    "        - **pix2pix**: Image-to-image translation using paired data which is **Limitation** (e.g., sketches to photos)\n",
    "            - Loss: L1 loss (structure), GAN loss (realism)\n",
    "            - Generator: U-Net (encoder–decoder + skip connections)\n",
    "            - Discriminator: PatchGAN (looks at patches, not whole image)\n",
    "    - **Unpaired image translation (CycleGAN)**: Learn to translate between two domains without paired examples using cycle consistency loss.\n",
    "        - Two generators (G: A→B, F: B→A) and two discriminators (D_A, D_B)\n",
    "        - Loss: Adversarial loss + Cycle consistency loss (F(G(A)) ≈ A, G(F(B)) ≈ B)\n",
    "        - Issues: Not controlled generation, hallucinations, not perfect\n",
    "\n",
    "**Why image translation matters**\n",
    "- colorization\n",
    "- inpainting\n",
    "- restoration\n",
    "- medical imaging\n",
    "- data augmentation\n",
    "\n",
    "- **Diffusion models (modern SOTA)**: Generate images by denoising noise step by step.\n",
    "    - **Based on**: Unet architecture + skip connections, timestep embeddings (tell network how noisy the image is)\n",
    "    - **Forward diffusion**: start with real image and add noise gradually over T steps until it becomes pure noise.\n",
    "    - **Reverse diffusion**: start with pure noise and learn to denoise step by step\n",
    "    - **Cons**: slow and expensive\n",
    "    - **Pros** (vs GANs): more stable training, better diversity, high-quality images, easier conditioning\n",
    "    - **Making efficient**:\n",
    "        - **low resolution** then **upsample**\n",
    "        - **latent diffusion**: encode image into latent space (embeddings), run diffusion in latent space then decode back to image\n",
    "    - **Control methods**:\n",
    "        - **Explicit conditioning**: Give text, image, input to train with condition\n",
    "        - **Classifier guidance**: use a classifier’s gradients and push generation toward desired class\n",
    "            - Need classifier trained on noisy images\n",
    "            - Bad gradients\n",
    "        - **Classifier-free guidance** (Most common): Train diffusion model with and without conditions then guide generation using difference\n",
    "            - Like: Stable Diffusion, DALL·E, GLIDE\n",
    "    - **Image editing with diffusion**\n",
    "        - **SDEdit**: add noise to an existing image, denoise with prompt and keeps structure, changes content\n",
    "        - **Prompt-to-Prompt**: modify attention maps, change parts of the image and keep rest fixed\n",
    "\n",
    "---\n",
    "\n",
    "| Model          | Strength         | Weakness    |\n",
    "| -------------- | ---------------- | ----------- |\n",
    "| Autoregressive | exact likelihood | slow        |\n",
    "| GAN            | sharp images     | unstable    |\n",
    "| Diffusion      | best quality     | slow, heavy |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
