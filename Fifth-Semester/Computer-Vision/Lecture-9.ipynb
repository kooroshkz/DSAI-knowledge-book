{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4ed6e4",
   "metadata": {},
   "source": [
    "- **Transformers** build upon:\n",
    "    - **Tokens**: vectores $\\to$ for image $\\to$ flaten patches turn to vectors\n",
    "    - **Attention Mechanism**: Finds relevant tokens and combines them in a new representation\n",
    "        - Each token transformed into three vectors: Query (What looking at), Key (What contains), Value (What information to give)\n",
    "        - Compare query with all keys to get similarity scores and turn scores to weights to weight vectors\\\n",
    "        - **Self-Attention**: when Query, Key, Value come from the same tokens, every patch can look at every other patch\n",
    "            - Links distant image regions, models global structure, undrestand relationships and helps with counting, comparison and layout\n",
    "            - CNN do this slowly through depth but transformers do it in one step\n",
    "            - **Multi-Head Attention**: multiple attention mechanisms in parallel to capture different relationships (shape,color,texture) and output is combined\n",
    "    - **Positional Encoding**: Adds information about the position of tokens in the sequence (Transformers are **Permutation equivariant:**) can be learned, sinusoidal or Fourier-based\n",
    "\n",
    "- **Vision Transformer (ViT)**: Split image into patches, project patches to tokens, add positional encodings, pass through transformer layers (multi-head self-attention + MLP), classify with a special [CLS] classification\n",
    "\n",
    "- **Transformers**: global context, weak inductive bias, need more data, memory heavy (attention is O(NÂ²)), great for multimodal learning\n",
    "- **CNNs**: strong inductive bias (locality), work well with less data, efficient, harder to model long-range relations\n",
    "\n",
    "- **Transformers for video**: Split each frame into patches, add temporal/spatial positional encodings, tokens now represnt space-time chunks\n",
    "    - Full attention over space and time is expensive so:\n",
    "    - **Structured Attention**: space-only or time-only attention in alternating layers\n",
    "\n",
    "- **CNN**: parallel, local, short-range\n",
    "- **RNN**: sequential, long-range, slow\n",
    "- **Transformer**: parallel, long-range, flexible, memory-heavy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
