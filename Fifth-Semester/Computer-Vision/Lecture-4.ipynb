{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4f4ad7",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "- **Cross-entropy Loss**: used with softmax for multi-class classification tasks.\n",
    "    - If the correct class has low probability → **high loss**\n",
    "    - If correct class has high probability → **low loss**\n",
    "    - $ L = - \\log(p_{correct}) $\n",
    "- **Hinge loss (SVM)**:\n",
    "    - Encourages correct class to be at least a margin away from incorrect classes.\n",
    "    - $ L = \\max(0, 1 - y \\cdot f(x)) $\n",
    "- **Why not** Squared Error for classification?\n",
    "    - Bad gradients\n",
    "    - Slower learning\n",
    "    - Poor probabilistic interpretation\n",
    "\n",
    "- **Gradient Decent**\n",
    "    - **Batch Gradient Descent**: uses the entire dataset to compute gradients. $\\to$ slow\n",
    "    - **Stochastic Gradient Descent (SGD)**: uses a **mini-batch** $\\to$ faster, noisy\n",
    "\n",
    "- **Regularization**: prevents overfitting by adding a penalty to the loss function.\n",
    "    - **L2 Regularization** (Ridge): penalizes large weights.\n",
    "    - **Early Stopping**: stop training when validation loss starts to increase.\n",
    "\n",
    "- **Activation Functions**:\n",
    "    - **Step Function**: $ f(x) = \\begin{cases} 1 & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases} $ Not differentiable $\\to$ Cannot use gradient decent.\n",
    "    - **ReLU**: $ f(x) = \\max(0, x) $ $\\to$ default choice, but make neurons die $\\to$ Leaky ReLU: Fixed dead neurons by allowing small negative slope.\n",
    "    - **Sigmoid**: $ f(x) = \\frac{1}{1 + e^{-x}} $ ouput [0, 1] $\\to$ vanishing gradient problem.\n",
    "    - **Tanh**: $ f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $ Better than sigmoid as it is zero-centered $\\to$ vanishing gradient problem still exists.\n",
    "\n",
    "- **Backpropagation**: algorithm to compute gradients efficiently using the chain rule.\n",
    "    - Forward pass: compute outputs\n",
    "    - Backward pass: compute gradients layer by layer from output to input.\n",
    "    - **Backprop with vectorization**: use matrix operations to speed up computations like **Jacobians**\n",
    "    - **Other usecases**:\n",
    "        - Feature visualization\n",
    "        - DeepDream: modify input image to maximize certain neuron activations.\n",
    "        - Adversarial attacks: slightly add noise to input to fool the network.\n",
    "\n",
    "- **Optimization tricks**:\n",
    "    - **Learning Rate Scheduling**: decrease learning rate over time.\n",
    "    - **Momentum**: Make gradient decent doesn't fall in pit (local minima) by adding a fraction of the previous update to the current update.\n",
    "    - **Adam Optimizer**: combines momentum and adaptive learning rates in scale for each parameter. $\\to$ Default choice"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
