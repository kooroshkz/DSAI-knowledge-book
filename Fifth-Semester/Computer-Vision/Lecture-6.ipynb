{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4f9a93",
   "metadata": {},
   "source": [
    "### Types of visual recognition tasks\n",
    "- Image Classification\n",
    "    - One label per image\n",
    "- Semantic Segmentation\n",
    "    - One label per pixel, make object boundaries selected\n",
    "    - Evaluation metric: Avg of Intersection over Union (IoU) = $IoU = \\frac{Intersection of Prediction and Ground Truth}{Union of Prediction and Ground Truth}$\n",
    "    - FCN: Take classification CNN and replace final FC layers with conv layers to output spatial map\n",
    "        - Issue: Output map is lower resolution than input image\n",
    "        - Solution: Upsample (Combine **what** deep fratures with **where** spatial info)\n",
    "        - Methods:\n",
    "            - Interpolation\n",
    "            - learned unsampling\n",
    "            - skip connections\n",
    "- Object Detection\n",
    "    - Find objects and draw bounding boxes\n",
    "    - Evaluation metric:\n",
    "        - IoU for box overlap\n",
    "        - Precision: how many detections are correct\n",
    "        - Recall: how many true objects are detected\n",
    "    - **Regress Boudning Boxes**: Predict 4 values (x, y, w, h) + class label for each object to make bounding box\n",
    "        - Loss function: Cross entropy for class + Squared error for box coordinates\n",
    "- Instance Segmentation (Semantic Segmentation + Object Detection)\n",
    "    - separate object instances and label pixel by pixel\n",
    "    - **Mask R-CNN**: Faster R-CNN + small FCN per object -> Fix misalignment with RoIAlign\n",
    "        - detect objects (Faster R-CNN)\n",
    "        - For each RoI, extract features and run small FCN to predict mask\n",
    "\n",
    "\n",
    "### Fully Convolutional Networks (FCNs)\n",
    "- All CNNs are fully convolutional but FCNs are FC convolutional with 1x1 conv kernels\n",
    "- Can work on any image size\n",
    "- Output is spatial map instead of single vector\n",
    "- Perfect for segmentation and detection tasks\n",
    "\n",
    "<img src=\"../../Files/fifth-semester/cv/13.png\" alt=\"FCN Architecture\" width=\"600\"/>\n",
    "\n",
    "- **Encoder**: Extracts features and reduces spatial dimensions (Conv + Pooling)\n",
    "- **Decoder**: Upsamples to original image size (Conv + Upsampling)\n",
    "- **Idea**: compress -> understand -> decompress\n",
    "\n",
    "#### U-Net\n",
    "- Symmetric encoder-decoder architecture with skip connections\n",
    "- Wit **Skip Connections**, image will be encoded and undrestood and after upsampling the skip connections will help to recover spatial details lost during downsampling\n",
    "\n",
    "<img src=\"../../Files/fifth-semester/cv/14.png\" alt=\"U-Net Architecture\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Histograms of oriented gradients (HOG)\n",
    "\n",
    "Used for object detection, especially for pedestrians.\n",
    "- Divide image into small connected regions (cells)\n",
    "- For each cell, compute histogram of gradient directions or edge orientations for pixels within the cell\n",
    "- Normalize histograms across larger regions (blocks) for illumination invariance\n",
    "- Combine histograms to form feature vector for the image\n",
    "- Use SVM or other classifiers on HOG features for object detection\n",
    "\n",
    "<img src=\"../../Files/fifth-semester/cv/15.png\" alt=\"HOG Features\" width=\"600\"/>\n",
    "\n",
    "#### Region-based CNN (R-CNN) and Selective Search\n",
    "\n",
    "- R-CNN: Generate region proposals using Selective Search, merge similar regions and classify only those regions using CNN\n",
    "- Fast R-CNN: Run CNN once on full image to get feature map, map regions to feature map, classify and regress boxes\n",
    "\n",
    "<img src=\"../../Files/fifth-semester/cv/16.png\" alt=\"R-CNN Architecture\" width=\"600\"/>\n",
    "\n",
    "This R-CNN goes forward and backward for each region proposal, which is slow.\n",
    "\n",
    "#### Anchors\n",
    "\n",
    "Instead of predicting boxes from scratch use refrence boxes (anchors) of different sizes and aspect ratios at each location in the feature map. The model predicts offsets to these anchors to get final bounding boxes.\n",
    "\n",
    "#### Faster R-CNN\n",
    "\n",
    "Uses a Region Proposal Network (RPN) to generate region proposals directly from the feature map, sharing computation with the detection network for speed.\n",
    "\n",
    "They use **Feature Pyramid Networks (FPN)** to detect objects at different scales by building a pyramid of feature maps with different resolutions.\n",
    "- Image pyramid: slow\n",
    "- Feature pyramid: faster\n",
    "- It use CNN feature hierarchy combine shallow (where) + deep (what)\n",
    "\n",
    "<img src=\"../../Files/fifth-semester/cv/17.png\" alt=\"Anchors\" width=\"600\"/>\n",
    "<img src=\"../../Files/fifth-semester/cv/18.png\" alt=\"Anchor Boxes\" width=\"600\"/>\n",
    "\n",
    "Later **Feature pyramid networks (FPN)** use multi-scale feature maps to detect objects of different sizes faster.\n",
    "\n",
    "#### Non-Maximum Suppression (NMS)\n",
    "\n",
    "Keep highest scoring box and remove boxes with high IoU overlap with it. Repeat for remaining boxes as we have many overlapping boxes for same object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7129e67",
   "metadata": {},
   "source": [
    "- **Two-stage** detectors: Directly predict bounding boxes and class probabilities from full images in one pass, making them faster but sometimes less accurate\n",
    "    - R-CNN, Fast R-CNN, Faster R-CNN\n",
    "- **Single-stage** detectors: \n",
    "    - YOLO, SSD, RetinaNet"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
