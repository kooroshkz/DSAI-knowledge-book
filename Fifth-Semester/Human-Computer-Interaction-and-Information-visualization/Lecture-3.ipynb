{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4c0f1e",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ”„ Recap of Lecture 2\n",
    "\n",
    "* Memory (STM, LTM, Model Human Processor).\n",
    "* Closure, user attitude, anxiety.\n",
    "* Control & focus.\n",
    "* Attention, locus of attention.\n",
    "* Emotion & affect.\n",
    "* Key concepts: **usability** + **cognetics**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Reasoning & Learning\n",
    "\n",
    "* **Reasoning types**:\n",
    "\n",
    "  * *Deductive*: logical conclusion (if A â†’ B).\n",
    "  * *Abductive*: infer cause from event (can be wrong).\n",
    "  * *Inductive*: generalize from cases (not always true, but useful).\n",
    "* **Learning strategies**:\n",
    "\n",
    "  * *Behaviorism*: learning measured by behavior, not thought process.\n",
    "  * *Gestalt*: perception shaped by past experience, grouping info makes it meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘ï¸ Perception Basics\n",
    "\n",
    "* **Definition**: sensing (physical) + interpreting (cognitive).\n",
    "* **Sensation**: raw signals (sight, sound, touch).\n",
    "* **Perception**: meaning built from signals, shaped by context & memory.\n",
    "* Example: we see â€œthe catâ€ even if letters are scrambled (â€œtae chtâ€).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¨ Gestalt Principles (pattern perception)\n",
    "\n",
    "Humans naturally organize stimuli into patterns:\n",
    "\n",
    "1. **Pragnanz** â€“ simplest interpretation.\n",
    "2. **Proximity** â€“ nearby objects grouped.\n",
    "3. **Similarity** â€“ similar items grouped (shape, color).\n",
    "4. **Closure** â€“ missing info is filled in.\n",
    "5. **Continuity** â€“ connected elements seen as continuous.\n",
    "6. **Symmetry** â€“ balanced shapes grouped.\n",
    "7. **Common fate** â€“ moving elements grouped.\n",
    "8. **Familiarity** â€“ known patterns recognized easily.\n",
    "\n",
    "ğŸ‘‰ Used in GUI design, logos, icons, InfoVis.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘“ Human Vision\n",
    "\n",
    "* We prefer **whole patterns** over parts.\n",
    "* **Retina**:\n",
    "\n",
    "  * Cones (6â€“7M) â†’ color, detail (bright light, fovea).\n",
    "  * Rods (75â€“150M) â†’ dim light, motion.\n",
    "* **Visual field**: sharp center, blurred periphery, motion-only edges.\n",
    "\n",
    "### Color perception\n",
    "\n",
    "* 3 cone types: red, green, blue.\n",
    "* Eye most sensitive to **green/yellow**, weakest in **blue**.\n",
    "* **Color blindness** (mostly male, 5â€“8%): red/green most common.\n",
    "* **Color properties**: hue (name), saturation (intensity), lightness (value).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘ï¸ Vision & Perception Models\n",
    "\n",
    "* **Stage 1: Pre-attentive** (fast, parallel, automatic â†’ color, shape, saliency).\n",
    "* **Stage 2: Attribute attention** (focus, top-down, slower â†’ reasoning, memory).\n",
    "* **Stage 3: Active vision** (visual thinking, queries, inferences).\n",
    "\n",
    "ğŸ‘‰ In InfoVis:\n",
    "\n",
    "* Use pre-attentive features (color, shape) for **fast search**.\n",
    "* Avoid overload â†’ â€œless is more.â€\n",
    "* Use smart symbols for complex searches.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Depth Perception Cues (8)\n",
    "\n",
    "1. Stereoscopy (binocular vision).\n",
    "2. Accommodation (lens focus).\n",
    "3. Motion parallax (near = faster).\n",
    "4. Occlusion (near blocks far).\n",
    "5. Texture (detail decreases with distance).\n",
    "6. Familiarity (known object size).\n",
    "7. Perspective (parallel lines converge).\n",
    "8. Shadows.\n",
    "\n",
    "ğŸ‘‰ Very useful in InfoVis (3D charts, VR, spatial design).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘‚ Auditory Perception\n",
    "\n",
    "* Hearing = detecting sound waves.\n",
    "* Audition = making meaning from sound.\n",
    "* Sound properties: frequency (Hz), loudness (dB).\n",
    "* Range: 20 Hz â€“ 20k Hz, comfortable at 20â€“70 dB.\n",
    "* Gestalt applies to sound too (grouping, patterns).\n",
    "* Features:\n",
    "\n",
    "  * Transient (disappears if not continuous).\n",
    "  * Pervasive (we hear even if not looking).\n",
    "  * Grabs attention strongly.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‹ Haptics (Touch Perception)\n",
    "\n",
    "* **Definition**: bodyâ€™s sensitivity to surroundings through touch/movement.\n",
    "* **Types**:\n",
    "\n",
    "  * Proprioceptive â†’ body position.\n",
    "  * Vestibular â†’ balance, acceleration.\n",
    "  * Kinaesthetic â†’ motion/force sense.\n",
    "  * Cutaneous â†’ skin (pressure, pain, temp).\n",
    "  * Tactile â†’ touch/pressure.\n",
    "* **Force feedback** â†’ used in gaming, VR, prosthetics.\n",
    "* Examples: iPhone â€œforce touchâ€, DARPA prosthetics with real touch feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Design Implications\n",
    "\n",
    "* Knowledge of perception helps design **effective interfaces**.\n",
    "* Ignoring perception principles = **dysfunctional systems**.\n",
    "* Key:\n",
    "\n",
    "  * Use Gestalt grouping for clarity.\n",
    "  * Respect limits of vision, color, memory.\n",
    "  * Support focus (avoid distractions).\n",
    "  * Use sound + haptics wisely for feedback."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
