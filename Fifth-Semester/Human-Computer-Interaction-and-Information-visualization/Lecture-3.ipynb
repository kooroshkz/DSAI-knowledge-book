{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4c0f1e",
   "metadata": {},
   "source": [
    "\n",
    "## 🔄 Recap of Lecture 2\n",
    "\n",
    "* Memory (STM, LTM, Model Human Processor).\n",
    "* Closure, user attitude, anxiety.\n",
    "* Control & focus.\n",
    "* Attention, locus of attention.\n",
    "* Emotion & affect.\n",
    "* Key concepts: **usability** + **cognetics**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Reasoning & Learning\n",
    "\n",
    "* **Reasoning types**:\n",
    "\n",
    "  * *Deductive*: logical conclusion (if A → B).\n",
    "  * *Abductive*: infer cause from event (can be wrong).\n",
    "  * *Inductive*: generalize from cases (not always true, but useful).\n",
    "* **Learning strategies**:\n",
    "\n",
    "  * *Behaviorism*: learning measured by behavior, not thought process.\n",
    "  * *Gestalt*: perception shaped by past experience, grouping info makes it meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## 👁️ Perception Basics\n",
    "\n",
    "* **Definition**: sensing (physical) + interpreting (cognitive).\n",
    "* **Sensation**: raw signals (sight, sound, touch).\n",
    "* **Perception**: meaning built from signals, shaped by context & memory.\n",
    "* Example: we see “the cat” even if letters are scrambled (“tae cht”).\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 Gestalt Principles (pattern perception)\n",
    "\n",
    "Humans naturally organize stimuli into patterns:\n",
    "\n",
    "1. **Pragnanz** – simplest interpretation.\n",
    "2. **Proximity** – nearby objects grouped.\n",
    "3. **Similarity** – similar items grouped (shape, color).\n",
    "4. **Closure** – missing info is filled in.\n",
    "5. **Continuity** – connected elements seen as continuous.\n",
    "6. **Symmetry** – balanced shapes grouped.\n",
    "7. **Common fate** – moving elements grouped.\n",
    "8. **Familiarity** – known patterns recognized easily.\n",
    "\n",
    "👉 Used in GUI design, logos, icons, InfoVis.\n",
    "\n",
    "---\n",
    "\n",
    "## 👓 Human Vision\n",
    "\n",
    "* We prefer **whole patterns** over parts.\n",
    "* **Retina**:\n",
    "\n",
    "  * Cones (6–7M) → color, detail (bright light, fovea).\n",
    "  * Rods (75–150M) → dim light, motion.\n",
    "* **Visual field**: sharp center, blurred periphery, motion-only edges.\n",
    "\n",
    "### Color perception\n",
    "\n",
    "* 3 cone types: red, green, blue.\n",
    "* Eye most sensitive to **green/yellow**, weakest in **blue**.\n",
    "* **Color blindness** (mostly male, 5–8%): red/green most common.\n",
    "* **Color properties**: hue (name), saturation (intensity), lightness (value).\n",
    "\n",
    "---\n",
    "\n",
    "## 👁️ Vision & Perception Models\n",
    "\n",
    "* **Stage 1: Pre-attentive** (fast, parallel, automatic → color, shape, saliency).\n",
    "* **Stage 2: Attribute attention** (focus, top-down, slower → reasoning, memory).\n",
    "* **Stage 3: Active vision** (visual thinking, queries, inferences).\n",
    "\n",
    "👉 In InfoVis:\n",
    "\n",
    "* Use pre-attentive features (color, shape) for **fast search**.\n",
    "* Avoid overload → “less is more.”\n",
    "* Use smart symbols for complex searches.\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Depth Perception Cues (8)\n",
    "\n",
    "1. Stereoscopy (binocular vision).\n",
    "2. Accommodation (lens focus).\n",
    "3. Motion parallax (near = faster).\n",
    "4. Occlusion (near blocks far).\n",
    "5. Texture (detail decreases with distance).\n",
    "6. Familiarity (known object size).\n",
    "7. Perspective (parallel lines converge).\n",
    "8. Shadows.\n",
    "\n",
    "👉 Very useful in InfoVis (3D charts, VR, spatial design).\n",
    "\n",
    "---\n",
    "\n",
    "## 👂 Auditory Perception\n",
    "\n",
    "* Hearing = detecting sound waves.\n",
    "* Audition = making meaning from sound.\n",
    "* Sound properties: frequency (Hz), loudness (dB).\n",
    "* Range: 20 Hz – 20k Hz, comfortable at 20–70 dB.\n",
    "* Gestalt applies to sound too (grouping, patterns).\n",
    "* Features:\n",
    "\n",
    "  * Transient (disappears if not continuous).\n",
    "  * Pervasive (we hear even if not looking).\n",
    "  * Grabs attention strongly.\n",
    "\n",
    "---\n",
    "\n",
    "## ✋ Haptics (Touch Perception)\n",
    "\n",
    "* **Definition**: body’s sensitivity to surroundings through touch/movement.\n",
    "* **Types**:\n",
    "\n",
    "  * Proprioceptive → body position.\n",
    "  * Vestibular → balance, acceleration.\n",
    "  * Kinaesthetic → motion/force sense.\n",
    "  * Cutaneous → skin (pressure, pain, temp).\n",
    "  * Tactile → touch/pressure.\n",
    "* **Force feedback** → used in gaming, VR, prosthetics.\n",
    "* Examples: iPhone “force touch”, DARPA prosthetics with real touch feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Design Implications\n",
    "\n",
    "* Knowledge of perception helps design **effective interfaces**.\n",
    "* Ignoring perception principles = **dysfunctional systems**.\n",
    "* Key:\n",
    "\n",
    "  * Use Gestalt grouping for clarity.\n",
    "  * Respect limits of vision, color, memory.\n",
    "  * Support focus (avoid distractions).\n",
    "  * Use sound + haptics wisely for feedback."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
